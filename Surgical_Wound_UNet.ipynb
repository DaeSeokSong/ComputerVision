{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaeSeokSong/image-processing/blob/master/Surgical_Wound_UNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuju0_8jyNnP"
      },
      "source": [
        "# **Error Solution**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "\"\"\"\n",
        "Error solution 1.\n",
        "\n",
        "RuntimeError: CUDA error: no kernel image is available for execution on the device\n",
        "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
        "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
        "\n",
        "[https://min23th.tistory.com/22]\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Error solution 2.\n",
        "\n",
        "This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
        "\"\"\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Error solution 3.\n",
        "\n",
        "RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
        "\"\"\"\n",
        "loss.requires_grad_(True)\n",
        "```"
      ],
      "metadata": {
        "id": "rkZNskd6eB9v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GY1e9FPDm6F"
      },
      "source": [
        "# **Next step**\n",
        "\n",
        "1. UNet++ 참조하여 모델 개선\n",
        "2. ResUNet++ 참조하여 모델 개선"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETKXZy_BpRBn"
      },
      "source": [
        "# ***Reference***\n",
        "\n",
        "* *Paper*\n",
        ">   [U-net](https://paperswithcode.com/paper/u-net-convolutional-networks-for-biomedical)\n",
        ">\n",
        "> 기존 CNN은 Single classification task에 사용되었지만,\n",
        "> \n",
        "> biomedical image processing 분야에서는 한 이미지 내의 모든 pixel을 classification 하는 Semantic segmentation task가 중요하게 사용되었다.\n",
        ">\n",
        ">  sliding window 방식을 사용하는 CNN 구조와 달리 검증된 patch는 넘기기 때문에 보다 빠른 처리가 가능한 구조이다.\n",
        "> \n",
        "> 적은 양의 데이터로도 dataset argumentation을 통해 잘 학습시킬 수 있다.\n",
        ">\n",
        ">   * [U-net++](https://paperswithcode.com/paper/unet-a-nested-u-net-architecture-for-medical)\n",
        ">   * [ResUNet++](https://paperswithcode.com/paper/resunet-an-advanced-architecture-for-medical)\n",
        "\n",
        "<br>\n",
        "\n",
        "* *Lecture*\n",
        "> * [UNet architecture by pytorch](https://89douner.tistory.com/300)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgbE_AiY8yV7"
      },
      "source": [
        "# **1.Development enviroment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVNq8Y-A89nl"
      },
      "source": [
        "## *1) Import*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vKiIgjo9Dro"
      },
      "source": [
        "### 1-1) Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZsXjyWownhef"
      },
      "outputs": [],
      "source": [
        "# U-net\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Image processing\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pickle as pl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import output\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "# ETC\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcj_F7KZ9FEV"
      },
      "source": [
        "### 1-2) Mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1I8TPjO9INs",
        "outputId": "e3fc408a-3421-4bfd-e4a4-0e677734b55a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrRWaQd09Joq",
        "outputId": "59d3fbc2-e25f-4fe0-9281-76c6fcaada89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Models/Surgical-Wound_Segmentation\n",
            "total 253\n",
            "drwx------ 2 root root  4096 Sep 14 08:50 'Case Report'\n",
            "drwx------ 2 root root  4096 Aug 16 08:53  Dataset\n",
            "drwx------ 2 root root  4096 Sep 25 16:33 '# Lagacy'\n",
            "drwx------ 2 root root  4096 Aug 17 05:49  Log\n",
            "-rw------- 1 root root 67999 Sep 29 17:50 '[Model Tester] Down Sampling.ipynb'\n",
            "-rw------- 1 root root 85905 Sep 29 12:28  Preprocessor.ipynb\n",
            "drwx------ 2 root root  4096 Sep 14 06:04 'Raw Dataset'\n",
            "-rw------- 1 root root 84030 Sep 29 17:59 'Surgical-Wound UNet.ipynb'\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/MyDrive/Models/Surgical-Wound_Segmentation\n",
        "!ls -al"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGNH-Msw9Lyx"
      },
      "source": [
        "# **2.Train U-Net**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V4ug77i9eo6"
      },
      "source": [
        "## *1) Grobal variable*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IfyygS-p9gzc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43cdfb2e-831e-413c-f2e3-ad2e271519f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU use:  True\n",
            "\n",
            "Thu Sep 29 17:59:55 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8    10W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Path\n",
        "MODEL_PATH = \"/content/gdrive/MyDrive/Models/Surgical-Wound_Segmentation\"\n",
        "\n",
        "DATASET_PATH = \"/Dataset\"\n",
        "LOG_PATH = \"/Log\"\n",
        "CHECK_POINT_PATH = '/CheckPoint'\n",
        "\n",
        "TRAIN_PATH = \"/train\"\n",
        "VAL_PATH = \"/val\"\n",
        "TEST_PATH = \"/test\"\n",
        "\n",
        "# Model parameter\n",
        "NORM_INPUT_W_SIZE = 224\n",
        "NORM_INPUT_H_SIZE = 224\n",
        "\n",
        "LR = 1e-3\n",
        "BATCH_SIZE = 8 # batch size 8\n",
        "EPOCHS = 10 # basic == 100\n",
        "\n",
        "\"\"\"\n",
        "GPU 사용이 가능하면 cuda 사용\n",
        "아니면 CPU를 이용하여 학습\n",
        "\"\"\"\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"GPU use: \", torch.cuda.is_available())\n",
        "print()\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtN2YVKdzEo0"
      },
      "source": [
        "## *2) Funtion*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNXX7XmzGTEa"
      },
      "source": [
        "### 2-1) Save learned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QUyzWtquGTPj"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "[Pytorch model save tutorial]\n",
        ">> https://tutorials.pytorch.kr/beginner/saving_loading_models.html\n",
        "\"\"\"\n",
        "\n",
        "# Save network\n",
        "def save(ckpt_dir, net, opt, epoch):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        os.makedirs(ckpt_dir)\n",
        "\n",
        "    torch.save({'net':net.state_dict(), 'opt':opt.state_dict()}, \n",
        "               ckpt_dir + '/model_epoch_%d.pt' %(epoch))\n",
        "\n",
        "# Load network\n",
        "def load(ckpt_dir, net, opt):\n",
        "    if not os.path.exists(ckpt_dir): # 저장된 네트워크가 없다면 인풋을 그대로 반환\n",
        "        return net, opt\n",
        "\n",
        "    lst_ckpt = os.listdir(ckpt_dir) # ckpt_dir 아래 있는 모든 파일 리스트를 받아온다\n",
        "    if len(lst_ckpt) == 0 : # 저장된 모델이 없으면 인풋 그대로 반환\n",
        "        return net, opt\n",
        "\n",
        "    lst_ckpt.sort(key = lambda f : int(f.split('.')[0].split('_')[-1]))\n",
        "    dict_model = torch.load('%s/%s' % (ckpt_dir, lst_ckpt[-1]))\n",
        "\n",
        "    net.load_state_dict(dict_model['net'])\n",
        "    opt.load_state_dict(dict_model['opt'])\n",
        "\n",
        "    return net, opt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-2) Down sampling func"
      ],
      "metadata": {
        "id": "fXTqCOTFVQ3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_padding(image, gt_image):\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) * 255\n",
        "\n",
        "    # Find contours\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    # Crop ROI\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    gt_image = gt_image[y:y+height, x:x+width]\n",
        "\n",
        "    return image, gt_image\n",
        "\n",
        "def get_divisors(n):\n",
        "    data = []\n",
        "\n",
        "    # 약수\n",
        "    divisor = 2\n",
        "\n",
        "    # 2의 배수만을 약수로 취급\n",
        "    while(divisor <= n):\n",
        "        if n % divisor == 0:\n",
        "            data.append(divisor)\n",
        "\n",
        "        divisor = divisor * 2\n",
        "            \n",
        "    return data\n",
        "\n",
        "def down_sampling(image, gt_image):\n",
        "    # Easy Max Adaptive Sliding Window\n",
        "    # 1. feature_size의 약수 중 2의 배수를 계산하여 divisors에 저장한다.\n",
        "    # 2. divisors 안의 숫자 중 가장 큰수를 골라 stride라고 정의한다.\n",
        "    # 3. 기준 축(anchor_axis)이 아닌 축의 길이를 slide_size라고 정의한다.\n",
        "    # 4. (slide_size-feature_size) // stride를 계산하여 step_num으로 정의한다.\n",
        "    # 5. (slide_size-feature_size) % stride를 계산하여 remain_pixel으로 정의한다.\n",
        "    # 6. remain_pixel을 stride에 더할 extra_idx를 random으로 정한다.\n",
        "    # 7. extra_random_stride != 0이라면, 각 stride에 맞게 slide window를 진행하면서 랜덤하게 적어도 1번은 해당 step의 stride에 extra_random_stride를 더한다.\n",
        "    # 8. window ROI의 (전체 픽셀-noneZeroCount)를 비교하여 gt의 흰색 픽셀 비율이 10% 미만이면 건너뛴다.\n",
        "\n",
        "    # 0.\n",
        "    feature_size = 224\n",
        "\n",
        "    # 1.\n",
        "    divisors = get_divisors(224)\n",
        "\n",
        "    # 2.\n",
        "    stride = max(divisors)\n",
        "    height, width, channel = image.shape\n",
        "\n",
        "    # 3.\n",
        "    if height > width:\n",
        "        assert width == feature_size, \"Preprocessed image width is wrong\"\n",
        "\n",
        "        anchor_axis = 'x'\n",
        "        slide_size = height\n",
        "    else:\n",
        "        assert height == feature_size, \"Preprocessed image width is height\"\n",
        "            \n",
        "        anchor_axis = 'y'\n",
        "        slide_size = width\n",
        "\n",
        "    gt_pixel_num = cv2.countNonZero(gt_image)\n",
        "    if gt_pixel_num == 0 : return [], [], anchor_axis, {}\n",
        "\n",
        "    # 4.\n",
        "    step_num = (slide_size - feature_size) // stride\n",
        "    if step_num < 1: return [], [], anchor_axis, {}\n",
        "\n",
        "    # 5.\n",
        "    remain_pixel = (slide_size - feature_size) % stride\n",
        "\n",
        "    # 6.\n",
        "    extra_idx = random.randrange(1, (step_num + 1))\n",
        "\n",
        "    # 7.\n",
        "    images = []\n",
        "    gt_images = []\n",
        "    strides = {}\n",
        "    for step in range(0, step_num+1):\n",
        "        tmp_stride = stride\n",
        "\n",
        "        if step == extra_idx:\n",
        "            tmp_stride += remain_pixel\n",
        "\n",
        "        if step == 0:\n",
        "            strides[step] = 0\n",
        "        else:\n",
        "            strides[step] = tmp_stride\n",
        "\n",
        "        start_idx = tmp_stride * step\n",
        "        end_idx = (tmp_stride * step) + feature_size\n",
        "        if step > extra_idx:\n",
        "            start_idx += remain_pixel\n",
        "            end_idx += remain_pixel\n",
        "\n",
        "        if anchor_axis == 'x':\n",
        "            slide_gt = gt_image[start_idx:end_idx, :]\n",
        "            gt_h, gt_w = slide_gt.shape\n",
        "\n",
        "            if gt_h < 224: continue\n",
        "\n",
        "            cnt_white_px = cv2.countNonZero(slide_gt)\n",
        "\n",
        "            # 8.\n",
        "            if cnt_white_px / gt_pixel_num > 0.1:\n",
        "                images.append(image[start_idx:end_idx, :, :])\n",
        "                gt_images.append(slide_gt)\n",
        "\n",
        "        elif anchor_axis == 'y':\n",
        "            slide_gt = gt_image[:, start_idx:end_idx]\n",
        "            gt_h, gt_w = slide_gt.shape\n",
        "\n",
        "            if gt_w < 224: continue\n",
        "\n",
        "            cnt_white_px = cv2.countNonZero(slide_gt)\n",
        "\n",
        "            # 8.\n",
        "            if cnt_white_px / gt_pixel_num > 0.1:\n",
        "                images.append(image[:, start_idx:end_idx, :])\n",
        "                gt_images.append(slide_gt)\n",
        "\n",
        "        else:\n",
        "            assert True, \"CODE ERROR, choice anchor axis\"\n",
        "\n",
        "    return images, gt_images, anchor_axis, strides"
      ],
      "metadata": {
        "id": "7Xgs48lIVRZ5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2-2-1) Concat sampled func"
      ],
      "metadata": {
        "id": "Zbqyypd6VYtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_gt_samples(gt_batchs, anchor_axis, strides):\n",
        "    if anchor_axis == 'x':\n",
        "        height = NORM_INPUT_H_SIZE + sum(strides.values())\n",
        "        concat_output = np.zeros((height, NORM_INPUT_W_SIZE), np.uint8)\n",
        "    elif anchor_axis == 'y':\n",
        "        width = NORM_INPUT_W_SIZE + sum(strides.values())\n",
        "        concat_output = np.zeros((NORM_INPUT_H_SIZE, width), np.uint8)\n",
        "    else:\n",
        "        assert anchor_axis == 'x' or anchor_axis == 'y', \"[ERROR] Set Wrong Anchor Axis\"\n",
        "\n",
        "    alpha = 0.5\n",
        "    total_stride = 0\n",
        "    for step, gt_images in enumerate(gt_batchs):\n",
        "        stride = strides[step]\n",
        "        total_stride += stride\n",
        "\n",
        "        for gt in gt_images:\n",
        "            gt = gt.to('cpu').detach().numpy()\n",
        "            \n",
        "            if step == 0:\n",
        "                if anchor_axis == 'x':\n",
        "                    concat_output[0:NORM_INPUT_H_SIZE, :] = gt[0]\n",
        "                if anchor_axis == 'y':\n",
        "                    concat_output[: ,0:NORM_INPUT_W_SIZE] = gt[0]\n",
        "            else:\n",
        "                if anchor_axis == 'x':\n",
        "                    concat_output_roi = concat_output[total_stride:total_stride+NORM_INPUT_H_SIZE-stride, :]\n",
        "                    concat_gt = gt[0][0:NORM_INPUT_H_SIZE-stride, :]\n",
        "\n",
        "                    concat_output_roi = cv2.addWeighted(\n",
        "                        concat_output_roi, alpha,\n",
        "                        concat_gt, (1-alpha), 0, dtype = cv2.CV_8UC1)\n",
        "                    \n",
        "                    concat_output[total_stride+(NORM_INPUT_H_SIZE-stride):total_stride+NORM_INPUT_H_SIZE, :] = gt[0][NORM_INPUT_H_SIZE-stride-1:-1, :]\n",
        "                if anchor_axis == 'y':\n",
        "                    concat_output_roi = concat_output[:, total_stride:total_stride+NORM_INPUT_W_SIZE-stride]\n",
        "                    concat_gt = gt[0][:, 0:NORM_INPUT_W_SIZE-stride]\n",
        "\n",
        "                    concat_output_roi = cv2.addWeighted(\n",
        "                        concat_output_roi, alpha,\n",
        "                        concat_gt, (1-alpha), 0, dtype = cv2.CV_8UC1)\n",
        "                    \n",
        "                    concat_output[:, total_stride+(NORM_INPUT_W_SIZE-stride):total_stride+NORM_INPUT_W_SIZE] = gt[0][:, NORM_INPUT_W_SIZE-stride-1:-1]\n",
        "\n",
        "    return concat_output"
      ],
      "metadata": {
        "id": "s8YbEdeMVdky"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrmqoZrZzGz5"
      },
      "source": [
        "### 2-3) Convenience func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "x7qxDCQnzGcm"
      },
      "outputs": [],
      "source": [
        "def imshow_waitkey_enter(image):\n",
        "    cv2_imshow(image)\n",
        "\n",
        "    time.sleep(0.5)\n",
        "    \n",
        "    input(\"Please press the Enter key to proceed\\n\")\n",
        "    output.clear()\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vin2caAL9b0r"
      },
      "source": [
        "## *3) Class*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7ttKi5fU8L6"
      },
      "source": [
        "### 3-1) Custom U-Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj6HFu6eUeuw"
      },
      "source": [
        "#### 3-1-1) Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYEyOHCoTsJV"
      },
      "source": [
        "**마지막 Output 채널 2 → 1 변경**\n",
        "> Binary classification\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=14CzAAaKv5v7pVfvugBRbD1xI4IuhmoyT\"  width = 640>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3-1-2) Build network"
      ],
      "metadata": {
        "id": "8uDSF-htR_88"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ERI7fzVOrkQ"
      },
      "source": [
        "##### 3-1-2-1) Custom Test UNet\n",
        "\n",
        "파라미터 문제로인한 자원 낭비 및 학습 속도 지연으로 수정 필요"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUWtaQKf9dJ0"
      },
      "outputs": [],
      "source": [
        "# torch.nn의 Module 클래스를 상속한, 커스텀 UNet 클래스\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # kernel size, stride, padding, bias는 거의 고정 >> predefine\n",
        "        def ConvBatchReLU_2d(in_ch, out_ch, k_size=3, stride=1, padding=0, bias=True):\n",
        "            layers = []\n",
        "\n",
        "            # Add Conv layer\n",
        "            layers += [nn.Conv2d(in_channels=in_ch,\n",
        "                                 out_channels=out_ch,\n",
        "                                 kernel_size=k_size,\n",
        "                                 stride=stride,\n",
        "                                 padding=padding,\n",
        "                                 bias=bias\n",
        "                                 )]\n",
        "\n",
        "            # Add batch normalization layer\n",
        "            layers += [nn.BatchNorm2d(num_features=out_ch)]\n",
        "\n",
        "            # Add ReLU\n",
        "            layers += [nn.ReLU()]\n",
        "\n",
        "            # Define conv, ReLU step in contracting path\n",
        "            CBR = nn.Sequential(*layers)\n",
        "\n",
        "            return CBR\n",
        "\n",
        "        \"\"\"\n",
        "        [Contracting path]\n",
        "        >> 입력 이미지의 context 포착이 목적\n",
        "        \"\"\"\n",
        "        # enc == encoder / n_m == n번째 stage(step)의 m번째 레이어\n",
        "        self.enc1_1 = ConvBatchReLU_2d(in_ch=1, out_ch=64)\n",
        "        self.enc1_2 = ConvBatchReLU_2d(in_ch=64, out_ch=64)\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc2_1 = ConvBatchReLU_2d(in_ch=64, out_ch=128)\n",
        "        self.enc2_2 = ConvBatchReLU_2d(in_ch=128, out_ch=128)\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc3_1 = ConvBatchReLU_2d(in_ch=128, out_ch=256)\n",
        "        self.enc3_2 = ConvBatchReLU_2d(in_ch=256, out_ch=256)\n",
        "\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc4_1 = ConvBatchReLU_2d(in_ch=256, out_ch=512)\n",
        "        self.enc4_2 = ConvBatchReLU_2d(in_ch=512, out_ch=512)\n",
        "\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc5_1 = ConvBatchReLU_2d(in_ch=512, out_ch=1024)\n",
        "\n",
        "        \"\"\"\n",
        "        [Expansive path]\n",
        "        >> 세밀한 Localization을 위한 높은 차원의 채널을 갖는 Upsampling\n",
        "        >> 얕은 레이어의 특집 맵을 결합\n",
        "        \"\"\"\n",
        "        # dec == decoder\n",
        "        self.dec5_1 = ConvBatchReLU_2d(in_ch=1024, out_ch=512)\n",
        "\n",
        "        # up-conv 레이어는 채널을 복원을 해야하기 때문에 kernel size를\n",
        "        # 대칭되는 MaxPool layer의 kernel size와 같도록 설정한다.\n",
        "        self.unpool4 = nn.ConvTranspose2d(in_channels=512,\n",
        "                                          out_channels=512,\n",
        "                                          kernel_size=2,\n",
        "                                          stride=2,\n",
        "                                          padding=0,\n",
        "                                          bias=True)\n",
        "        \n",
        "        # input channel은 up-conv와 대칭되는 enc, 두 레이어에서\n",
        "        # 같은 크기의 채널로 오기 때문에 대칭 enc 레이어보다 input이 두 배 많다.\n",
        "        self.dec4_2 = ConvBatchReLU_2d(in_ch=2 * 512, out_ch=512)\n",
        "        self.dec4_1 = ConvBatchReLU_2d(in_ch=512, out_ch=256)\n",
        "\n",
        "        self.unpool3 = nn.ConvTranspose2d(in_channels=256,\n",
        "                                          out_channels=256,\n",
        "                                          kernel_size=2,\n",
        "                                          stride=2,\n",
        "                                          padding=0,\n",
        "                                          bias=True)\n",
        "        \n",
        "        self.dec3_2 = ConvBatchReLU_2d(in_ch=2 * 256, out_ch=256)\n",
        "        self.dec3_1 = ConvBatchReLU_2d(in_ch=256, out_ch=128)\n",
        "\n",
        "        self.unpool2 = nn.ConvTranspose2d(in_channels=128,\n",
        "                                          out_channels=128,\n",
        "                                          kernel_size=2,\n",
        "                                          stride=2,\n",
        "                                          padding=0,\n",
        "                                          bias=True)\n",
        "        \n",
        "        self.dec2_2 = ConvBatchReLU_2d(in_ch=2 * 128, out_ch=128)\n",
        "        self.dec2_1 = ConvBatchReLU_2d(in_ch=128, out_ch=64)\n",
        "\n",
        "        self.unpool1 = nn.ConvTranspose2d(in_channels=64,\n",
        "                                          out_channels=64,\n",
        "                                          kernel_size=2,\n",
        "                                          stride=2,\n",
        "                                          padding=0,\n",
        "                                          bias=True)\n",
        "        \n",
        "        self.dec1_2 = ConvBatchReLU_2d(in_ch=2 * 64, out_ch=64)\n",
        "        self.dec1_1 = ConvBatchReLU_2d(in_ch=64, out_ch=64)\n",
        "\n",
        "        # conv 1*1, N class for segmentation\n",
        "        # 이미지 상에서는 out_channels 2라 되어있으나 결과 도출을 위해 1로 설정\n",
        "        self.conv = nn.Conv2d(in_channels=64,\n",
        "                              out_channels=1,\n",
        "                              kernel_size=1,\n",
        "                              stride=1,\n",
        "                              padding=0,\n",
        "                              bias=True)\n",
        "\n",
        "    \"\"\"\n",
        "    [Skip connection]\n",
        "        >> Semantic segmentation에서는 위치정보가 중요하기에\n",
        "        >> 이에 대한 소실 방지 차원에서 이전 연산했던 값을 더해준다.\n",
        "\n",
        "        >> copy and crop\n",
        "        >> encoding 데이터가 더 크므로 복사(copy)후 잘라준다(crop)\n",
        "    \"\"\"\n",
        "    def copy_and_crop(self, enc, unpool):\n",
        "        cpy_enc =  enc.clone().cpu().detach().numpy()\n",
        "        diff_y = cpy_enc.shape[2] - unpool.shape[2] # height\n",
        "        diff_x = cpy_enc.shape[3] - unpool.shape[3] # width\n",
        "\n",
        "        top = int(diff_y / 2)\n",
        "        left = int(diff_x / 2)\n",
        "        height = int(cpy_enc.shape[2] - top)\n",
        "        width = int(cpy_enc.shape[3] - left)\n",
        "\n",
        "        if height - top > unpool.shape[2]:\n",
        "            height = height - ((height - top) - unpool.shape[2])\n",
        "        elif height - top < unpool.shape[2]:\n",
        "            height = height + (unpool.shape[2] - (height - top))\n",
        "\n",
        "        if width - left > unpool.shape[3]:\n",
        "            width = width - ((width - left) - unpool.shape[3])\n",
        "        elif width - left < unpool.shape[3]:\n",
        "            width = width + (unpool.shape[3] - (width - left))\n",
        "\n",
        "        cpy_enc = cpy_enc[:, :, top:height, left:width]\n",
        "        cpy_enc = torch.Tensor(cpy_enc).to(DEVICE)\n",
        "\n",
        "        #print(\"encoding size = \", cpy_enc.shape)\n",
        "        #print(\"unpooling size = \", unpool.shape)\n",
        "\n",
        "        return torch.cat((unpool, cpy_enc), dim=1)\n",
        "\n",
        "    # x == input_image\n",
        "    def forward(self, x):\n",
        "        #print(\"Input size = \", x.shape)\n",
        "        input_width, input_height = x.shape[2], x.shape[3]\n",
        "\n",
        "        # encoder part\n",
        "        enc1_1 = self.enc1_1(x)\n",
        "        enc1_2 = self.enc1_2(enc1_1)\n",
        "        pool1 = self.pool1(enc1_2)\n",
        "        #print(\"enc1_1 size = \", enc1_1.shape)\n",
        "        #print(\"enc1_2 size = \", enc1_2.shape)\n",
        "        #print(\"pool1 size = \", pool1.shape)\n",
        "\n",
        "        enc2_1 = self.enc2_1(pool1)\n",
        "        enc2_2 = self.enc2_2(enc2_1)\n",
        "        pool2 = self.pool2(enc2_2)\n",
        "        #print(\"enc2_1 size = \", enc2_1.shape)\n",
        "        #print(\"enc2_2 size = \", enc2_2.shape)\n",
        "        #print(\"pool2 size = \", pool2.shape)\n",
        "        \n",
        "        enc3_1 = self.enc3_1(pool2)\n",
        "        enc3_2 = self.enc3_2(enc3_1)\n",
        "        pool3 = self.pool3(enc3_2)\n",
        "        #print(\"enc3_1 size = \", enc3_1.shape)\n",
        "        #print(\"enc3_2 size = \", enc3_2.shape)\n",
        "        #print(\"pool3 size = \", pool3.shape)\n",
        "\n",
        "        enc4_1 = self.enc4_1(pool3)\n",
        "        enc4_2 = self.enc4_2(enc4_1)\n",
        "        pool4 = self.pool4(enc4_2)\n",
        "        #print(\"enc4_1 size = \", enc4_1.shape)\n",
        "        #print(\"enc4_2 size = \", enc4_2.shape)\n",
        "        #print(\"pool4 size = \", pool4.shape)\n",
        "\n",
        "        enc5_1 = self.enc5_1(pool4)\n",
        "        #print(\"enc5_1 size = \", enc5_1.shape)\n",
        "\n",
        "        # decoder part\n",
        "        dec5_1 = self.dec5_1(enc5_1)\n",
        "        #print(\"dec5_1 size = \", dec5_1.shape)\n",
        "\n",
        "        unpool4 = self.unpool4(dec5_1)\n",
        "        #print(\"unpool4 size = \", unpool4.shape)\n",
        "\n",
        "        cat4 = self.copy_and_crop(enc4_2, unpool4)\n",
        "        dec4_2 = self.dec4_2(cat4)\n",
        "        dec4_1 = self.dec4_1(dec4_2)\n",
        "        #print(\"cat4 size = \", cat4.shape)\n",
        "        #print(\"dec4_2 size = \", dec4_2.shape)\n",
        "        #print(\"dec4_1 size = \", dec4_1.shape)\n",
        "\n",
        "        unpool3 = self.unpool3(dec4_1)\n",
        "        #print(\"unpool3 size = \", unpool3.shape)\n",
        "\n",
        "        cat3 = self.copy_and_crop(enc3_2, unpool3)\n",
        "        dec3_2 = self.dec3_2(cat3)\n",
        "        dec3_1 = self.dec3_1(dec3_2)\n",
        "        #print(\"cat3 size = \", cat3.shape)\n",
        "        #print(\"dec3_2 size = \", dec3_2.shape)\n",
        "        #print(\"dec3_1 size = \", dec3_1.shape)\n",
        "\n",
        "        unpool2 = self.unpool2(dec3_1)\n",
        "        #print(\"unpool2 size = \", unpool2.shape)\n",
        "\n",
        "        cat2 = self.copy_and_crop(enc2_2, unpool2)\n",
        "        dec2_2 = self.dec2_2(cat2)\n",
        "        dec2_1 = self.dec2_1(dec2_2)\n",
        "        #print(\"cat2 size = \", cat2.shape)\n",
        "        #print(\"dec2_2 size = \", dec2_2.shape)\n",
        "        #print(\"dec2_1 size = \", dec2_1.shape)\n",
        "\n",
        "        unpool1 = self.unpool1(dec2_1)\n",
        "        #print(\"unpool1 size = \", unpool1.shape)\n",
        "\n",
        "        cat1 = self.copy_and_crop(enc1_2, unpool1)\n",
        "        dec1_2 = self.dec1_2(cat1)\n",
        "        dec1_1 = self.dec1_1(dec1_2)\n",
        "        #print(\"cat1 size = \", cat1.shape)\n",
        "        #print(\"dec1_2 size = \", dec1_2.shape)\n",
        "        #print(\"dec1_1 size = \", dec1_1.shape)\n",
        "\n",
        "        output = torch.sigmoid(self.conv(dec1_1))\n",
        "        #print(\"Output size = \", output.shape)\n",
        "        #print(\"Output = \", output)\n",
        "\n",
        "        \"\"\"\n",
        "        output_width, output_height = x.shape[2], x.shape[3]\n",
        "\n",
        "        # [mirroring extrapolation]\n",
        "        # input(572*572)/output(388*388) 사이즈가 다르므로 output에 \n",
        "        # mirroring extrapolation 기법으로 missing context 부분을 채운다.\n",
        "\n",
        "        pad_width = int((input_width - output_width) / 2)\n",
        "        pad_height = int((input_height - output_height) / 2)\n",
        "        extrapolation = []\n",
        "        for image in x:\n",
        "            #image = nn.functional.pad(image, (pad_width, pad_width, pad_height, pad_height), 'reflect') # 이미지, (좌, 우, 상, 하), 방법\n",
        "            image = nn.functional.pad(image, (pad_width, pad_width, pad_height, pad_height), 'constant', 0)\n",
        "            extrapolation.append(image)\n",
        "\n",
        "        output = torch.stack(extrapolation, dim=0).to(DEVICE)\n",
        "        #print(\"Mirrored padding output size = \", output.shape)\n",
        "        #print(\"Constant padding output size = \", output.shape)\n",
        "        \"\"\"\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3-1-2-2) Polar Res UNet++"
      ],
      "metadata": {
        "id": "mthffBM_Rnev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, device, in_channels=3, out_channels=1, init_features=32): # 컬러\n",
        "#    def __init__(self, device, in_channels=1, out_channels=1, init_features=32): # 흑백\n",
        "        super(UNet, self).__init__()\n",
        "        self.device = device\n",
        "        \n",
        "        features = init_features\n",
        "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(\n",
        "        features * 16, features * 8, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
        "        self.upconv3 = nn.ConvTranspose2d(\n",
        "        features * 8, features * 4, kernel_size=2, stride=2\n",
        "        )\n",
        "\n",
        "        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
        "        self.upconv2 = nn.ConvTranspose2d(\n",
        "        features * 4, features * 2, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
        "        self.upconv1 = nn.ConvTranspose2d(\n",
        "        features * 2, features, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
        "\n",
        "        self.conv = nn.Conv2d(\n",
        "        in_channels=features, out_channels=out_channels, kernel_size=1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))\n",
        "        enc3 = self.encoder3(self.pool2(enc2))\n",
        "        enc4 = self.encoder4(self.pool3(enc3))\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
        "\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat((dec2, enc2,), dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat((dec1, enc1,), dim=1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "\n",
        "        return torch.sigmoid(self.conv(dec1))\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        return nn.Sequential(\n",
        "        OrderedDict(\n",
        "            [\n",
        "                (\n",
        "                    name + \"conv1\",\n",
        "                    nn.Conv2d(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=features,\n",
        "                    kernel_size=3,\n",
        "                    padding=1,\n",
        "                    bias=False,\n",
        "                    ),\n",
        "                ),\n",
        "                (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
        "                (name + \"relu1\", nn.ReLU(inplace=True)),\n",
        "                (\n",
        "                    name + \"conv2\",\n",
        "                    nn.Conv2d(\n",
        "                    in_channels=features,\n",
        "                    out_channels=features,\n",
        "                    kernel_size=3,\n",
        "                    padding=1,\n",
        "                    bias=False,\n",
        "                    ),\n",
        "                ),\n",
        "                (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
        "                (name + \"relu2\", nn.ReLU(inplace=True)),\n",
        "            ]\n",
        "        ))"
      ],
      "metadata": {
        "id": "HrPeVhcmRoc8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfV8NkvMTfgC"
      },
      "source": [
        "#### 3-1-3) Dice loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ms3M3MlhTjGT"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Best dice score model of Lesion Segmentation on ISIC 2018\n",
        ">> Some of them, DiceLoss code\n",
        "\n",
        "https://github.com/marinbenc/medical-polar-training/blob/main/loss.py\n",
        "https://paperswithcode.com/paper/training-on-polar-image-transformations#code\n",
        "\"\"\"\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = 1.0\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        #assert y_pred.size() == y_true.size()\n",
        "        dscs = torch.zeros(y_pred.shape[1])\n",
        "\n",
        "        for i in range(y_pred.shape[1]):\n",
        "          y_pred_ch = y_pred[:, i].contiguous().view(-1)\n",
        "          y_true_ch = y_true[:, i].contiguous().view(-1)\n",
        "\n",
        "          intersection = (y_pred_ch * y_true_ch).sum()\n",
        "\n",
        "          dscs[i] = (2. * intersection + self.smooth) / (y_pred_ch.sum() + y_true_ch.sum() + self.smooth)\n",
        "\n",
        "        # dice_score = 1.0 - dice_loss\n",
        "        return 1.0 - torch.mean(dscs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxCDqnkVU_fm"
      },
      "source": [
        "### 3-2) Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkgZx5qbVJNx"
      },
      "source": [
        "#### 3-2-1) Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VqnUx025VKpg"
      },
      "outputs": [],
      "source": [
        "class WoundDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        lst_data = os.listdir(self.data_dir)\n",
        "\n",
        "        lst_wound = [f for f in lst_data if f.startswith('wound')]\n",
        "        lst_gt = [f for f in lst_data if f.startswith('gt')]\n",
        "\n",
        "        lst_wound.sort()\n",
        "        lst_gt.sort()\n",
        "\n",
        "        self.lst_wound = lst_wound\n",
        "        self.lst_gt = lst_gt\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lst_gt)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        wound = cv2.imread(os.path.join(self.data_dir, self.lst_wound[index]))\n",
        "        gt = cv2.imread(os.path.join(self.data_dir, self.lst_gt[index]),\n",
        "                          cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        if wound.ndim == 2:\n",
        "            wound = wound[:, :, np.newaxis]\n",
        "        if gt.ndim == 2:\n",
        "            gt = gt[:, :, np.newaxis]\n",
        "\n",
        "        data = {'wound': wound, 'gt': gt}\n",
        "\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "\n",
        "        return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rv22BEuok0D"
      },
      "source": [
        "#### 3-2-2) Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1FfoKGJZol9C"
      },
      "outputs": [],
      "source": [
        "class ToTensor(object):\n",
        "    def __call__(self, data):\n",
        "        wound, gt = data['wound'], data['gt']\n",
        "        wound = wound.transpose((2, 0, 1)).astype(np.float32)\n",
        "        gt = gt.transpose((2, 0, 1)).astype(np.float32)\n",
        "\n",
        "        data = {'wound': torch.from_numpy(wound), 'gt': torch.from_numpy(gt)}\n",
        "\n",
        "        return data\n",
        "\n",
        "# Dataset arugmentation\n",
        "class Normalization(object):\n",
        "    def __init__(self, mean=0.5, std=0.5):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, data):\n",
        "        wound, gt = data['wound'], data['gt']\n",
        "\n",
        "        wound = (wound - self.mean) / self.std\n",
        "        data = {'wound': wound, 'gt': gt}\n",
        "\n",
        "        return data\n",
        "\n",
        "class RandomFlip(object):\n",
        "    def __call__(self, data):\n",
        "        wound, gt = data['wound'], data['gt']\n",
        "\n",
        "        if np.random.rand() > 0.5:\n",
        "            wound = np.fliplr(wound)\n",
        "            gt = np.fliplr(gt)\n",
        "\n",
        "        if np.random.rand() > 0.5:\n",
        "            wound = np.flipud(wound)\n",
        "            gt = np.flipud(gt)\n",
        "\n",
        "        data = {'wound': wound, 'gt': gt}\n",
        "\n",
        "        return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsjfNZrh9Z8C"
      },
      "source": [
        "## *4) Run*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu1q7hsx9p9y"
      },
      "source": [
        "### 4-1) Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rxBFGyd89aps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63ebcac3-59c6-40d3-c946-c71c439862c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset =  581\n",
            "val_dataset =  250\n",
            "ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
            "train_loader =  73\n",
            "val_loader =  32\n"
          ]
        }
      ],
      "source": [
        "trans = transforms.Compose([RandomFlip(),\n",
        "                            ToTensor()])\n",
        "\n",
        "train_dataset = WoundDataset(data_dir=MODEL_PATH + DATASET_PATH + TRAIN_PATH, transform=trans)\n",
        "print(\"train_dataset = \", len(train_dataset))\n",
        "\n",
        "val_dataset = WoundDataset(data_dir=MODEL_PATH + DATASET_PATH + VAL_PATH, transform=trans)\n",
        "print(\"val_dataset = \", len(val_dataset))\n",
        "\n",
        "print(\"ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "print(\"train_loader = \", len(train_loader))\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "print(\"val_loader = \", len(val_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbdBhwie-Q1X"
      },
      "source": [
        "### 4-2) Set variable about training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Jcr35-y8-Rk4"
      },
      "outputs": [],
      "source": [
        "# 커스텀\n",
        "#net = UNet().to(DEVICE)\n",
        "\n",
        "# Polar Res UNet++\n",
        "net = UNet(DEVICE).to(DEVICE)\n",
        "\n",
        "# before used Loss, nn.BCEWithLogitsLoss().to(DEVICE)\n",
        "criterion = DiceLoss().to(DEVICE)\n",
        "\n",
        "# Set optimizer\n",
        "opt = torch.optim.Adam(net.parameters(), lr=LR)\n",
        "\n",
        "num_batch_train = len(train_dataset) / BATCH_SIZE\n",
        "num_batch_val = len(val_dataset) / BATCH_SIZE\n",
        "\n",
        "# Set lambda func\n",
        "fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n",
        "fn_denorm = lambda x, mean, std: (x * std) + mean\n",
        "fn_class = lambda x: 1.0 * (x > 0.5)\n",
        "\n",
        "# Set SummaryWriter\n",
        "if not os.path.exists(MODEL_PATH + LOG_PATH + TRAIN_PATH):\n",
        "    os.makedirs(MODEL_PATH + LOG_PATH + TRAIN_PATH)\n",
        "\n",
        "if not os.path.exists(MODEL_PATH + LOG_PATH + VAL_PATH):\n",
        "    os.makedirs(MODEL_PATH + LOG_PATH + VAL_PATH)\n",
        "\n",
        "writer_train = SummaryWriter(log_dir=MODEL_PATH + LOG_PATH + TRAIN_PATH)\n",
        "writer_val = SummaryWriter(log_dir=MODEL_PATH + LOG_PATH + VAL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd82Nd77WOuS"
      },
      "source": [
        "### 4-3) Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmmcOJqKWQzi"
      },
      "outputs": [],
      "source": [
        "# Load saved model\n",
        "ckpt_dir = MODEL_PATH + CHECK_POINT_PATH\n",
        "net, opt = load(ckpt_dir, net, opt)\n",
        "\n",
        "# Init prams\n",
        "start_epoch = 0\n",
        "best_train_loss = -1\n",
        "best_val_loss = -1\n",
        "\n",
        "# Early stopping\n",
        "before_val_loss = 0\n",
        "stop_cnt = 0\n",
        "is_early_stop = False\n",
        "\n",
        "# Wrtie start time\n",
        "start = time.time()\n",
        "\n",
        "for epoch in range(start_epoch+1, EPOCHS+1):\n",
        "    net.train()\n",
        "    loss_arr = []\n",
        "\n",
        "    for batch, data in enumerate(train_loader, 1):\n",
        "        # Load data\n",
        "        wound = data['wound'].to(DEVICE)\n",
        "        gt = data['gt'].to(DEVICE)\n",
        "\n",
        "        for sample_idx, image in enumerate(wound):\n",
        "            # Get image and ground-truth\n",
        "            origin_wound = image.cpu().numpy().transpose(1, 2, 0).astype('uint8')\n",
        "            origin_gt = gt[sample_idx].cpu().numpy().transpose(1, 2, 0).astype('uint8')\n",
        "\n",
        "            # Cut padding\n",
        "            target_wound, target_gt = cut_padding(origin_wound, origin_gt)\n",
        "\n",
        "            # Reszie smaller side(width or height) to 224 in proportion\n",
        "            get_h, get_w, get_ch = target_wound.shape\n",
        "\n",
        "            if get_w < get_h:\n",
        "                resize_scale = NORM_INPUT_W_SIZE / get_w\n",
        "            else:\n",
        "                resize_scale = NORM_INPUT_H_SIZE / get_h\n",
        "\n",
        "            target_wound = cv2.resize(target_wound,\n",
        "                                      (0, 0),\n",
        "                                      fx=resize_scale,\n",
        "                                      fy=resize_scale,\n",
        "                                      interpolation=cv2.INTER_CUBIC)\n",
        "            target_gt = cv2.resize(target_gt, \n",
        "                                   (0, 0),\n",
        "                                   fx=resize_scale,\n",
        "                                   fy=resize_scale,\n",
        "                                   interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "            # Down sampling\n",
        "            sampling_wound, sampling_gt, anchor_axis, strides = down_sampling(target_wound, target_gt)\n",
        "            if len(sampling_wound) < 1 or len(sampling_gt) < 1 or len(strides) < 1: \n",
        "                # Preprocess\n",
        "                    # Norm\n",
        "                origin_image = origin_wound/255.0\n",
        "                origin_gt = origin_gt/255.0\n",
        "                    # Expand dims\n",
        "                origin_image = np.expand_dims(origin_image, axis=0).transpose(0, 3, 1, 2).astype('uint8')\n",
        "                origin_gt = np.expand_dims(origin_gt, axis=0).transpose(0, 3, 1, 2).astype('uint8')\n",
        "                    # ToTensor\n",
        "                origin_image = torch.Tensor(origin_image).to(DEVICE)\n",
        "                origin_gt = torch.Tensor(origin_gt).to(DEVICE)\n",
        "\n",
        "                # Forward\n",
        "                output = net(origin_image)\n",
        "\n",
        "                # Backward\n",
        "                opt.zero_grad()\n",
        "\n",
        "                loss = criterion(output, origin_gt)\n",
        "                loss.requires_grad_(True)\n",
        "                loss.backward()\n",
        "\n",
        "                opt.step()\n",
        "\n",
        "                loss_arr += [loss.item()]\n",
        "                \n",
        "                break\n",
        "\n",
        "            # Concat output\n",
        "            sampling_outputs = []\n",
        "            for step, ds_image in enumerate(sampling_wound):\n",
        "                # Preprocess\n",
        "                    # Norm input\n",
        "                ds_image = ds_image/255.0\n",
        "                    # Expand dims\n",
        "                ds_image = np.expand_dims(ds_image, axis=0).transpose(0, 3, 1, 2)\n",
        "                    # ToTensor\n",
        "                ds_image = torch.Tensor(ds_image).to(DEVICE)\n",
        "\n",
        "                # Forward\n",
        "                ds_output = net(ds_image)\n",
        "                sampling_outputs.append(ds_output)\n",
        "\n",
        "            # Postprocess\n",
        "                # Norm ground-truth\n",
        "            target_gt = target_gt/255.0\n",
        "                # Concat outputs\n",
        "            output = concat_gt_samples(sampling_outputs, anchor_axis, strides)\n",
        "                # Expand dims\n",
        "            output = np.expand_dims(output, axis=0).transpose(1, 2, 0).astype('uint8')\n",
        "            target_gt = np.expand_dims(target_gt, axis=0).transpose(1, 2, 0).astype('uint8')\n",
        "                # ToTensor\n",
        "            output = torch.Tensor(output).to(DEVICE)\n",
        "            target_gt = torch.Tensor(target_gt).to(DEVICE)\n",
        "\n",
        "            # Backward\n",
        "            opt.zero_grad()\n",
        "\n",
        "            loss = criterion(output, target_gt)\n",
        "            loss.requires_grad_(True)\n",
        "            loss.backward()\n",
        "\n",
        "            opt.step()\n",
        "\n",
        "            loss_arr += [loss.item()]\n",
        "\n",
        "        # Save loss\n",
        "        loss_arr += [loss.item()]\n",
        "        print(\"=======================================================\")\n",
        "        print(f\"TRAIN || EPOCH {epoch :04d} | BATCH {batch : 04d} / {math.ceil(num_batch_train) : 04d} | DICE LOSS {np.mean(loss_arr) : .4f}\")\n",
        "        print(\"=======================================================\")\n",
        "\n",
        "        # Save best loss\n",
        "        if batch > math.ceil((len(train_dataset) / BATCH_SIZE) / 2) :\n",
        "            if best_train_loss == -1 or np.mean(loss_arr) < best_train_loss:\n",
        "                best_train_loss = np.mean(loss_arr)\n",
        "\n",
        "        # Save tensorboard\n",
        "        gt = fn_tonumpy(gt)\n",
        "        wound = fn_tonumpy(fn_denorm(wound, mean=0.5, std=0.5))\n",
        "        if len(output.shape) == 3:\n",
        "            output = output.to('cpu').detach().numpy()\n",
        "            output = np.expand_dims(output, axis=0)\n",
        "            output = fn_class(output)\n",
        "        else:\n",
        "            output = fn_tonumpy(fn_class(output))\n",
        "\n",
        "        writer_train.add_image('ground-truth', gt, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
        "        writer_train.add_image('wound', wound, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
        "        writer_train.add_image('output', output, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
        "\n",
        "    writer_train.add_scalar('loss', np.mean(loss_arr), epoch)\n",
        "    print(\"Write train loss, epoch \", epoch)\n",
        "    print()\n",
        "\n",
        "    # Early stopping\n",
        "    if before_val_loss != 0 and np.mean(loss_arr) > before_val_loss:\n",
        "        stop_cnt += 1\n",
        "\n",
        "        if stop_cnt >= 10:\n",
        "            is_early_stop = True\n",
        "    else:\n",
        "        stop_cnt = 0\n",
        "\n",
        "    # Validate\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        loss_arr = []\n",
        "\n",
        "        for batch, data in enumerate(val_loader, 1):\n",
        "            # Forward\n",
        "            wound = data['wound'].to(DEVICE)\n",
        "            gt = data['gt'].to(DEVICE)\n",
        "\n",
        "            for sample_idx, image in enumerate(wound):\n",
        "                # Get image and ground-truth\n",
        "                origin_wound = image.cpu().numpy().transpose(1, 2, 0).astype('uint8')\n",
        "                origin_gt = gt[sample_idx].cpu().numpy().transpose(1, 2, 0).astype('uint8')\n",
        "\n",
        "                # Cut padding\n",
        "                target_wound, target_gt = cut_padding(origin_wound, origin_gt)\n",
        "\n",
        "                # Reszie smaller side(width or height) to 224 in proportion\n",
        "                get_h, get_w, get_ch = target_wound.shape\n",
        "\n",
        "                if get_w < get_h:\n",
        "                    resize_scale = NORM_INPUT_W_SIZE / get_w\n",
        "                else:\n",
        "                    resize_scale = NORM_INPUT_H_SIZE / get_h\n",
        "\n",
        "                target_wound = cv2.resize(target_wound,\n",
        "                                        (0, 0),\n",
        "                                        fx=resize_scale,\n",
        "                                        fy=resize_scale,\n",
        "                                        interpolation=cv2.INTER_CUBIC)\n",
        "                target_gt = cv2.resize(target_gt, \n",
        "                                    (0, 0),\n",
        "                                    fx=resize_scale,\n",
        "                                    fy=resize_scale,\n",
        "                                    interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "                # Down sampling\n",
        "                sampling_wound, sampling_gt, anchor_axis, strides = down_sampling(target_wound, target_gt)\n",
        "                if len(sampling_wound) < 1 or len(sampling_gt) < 1 or len(strides) < 1: \n",
        "                    # Preprocess\n",
        "                        # Norm\n",
        "                    origin_image = origin_wound/255.0\n",
        "                    origin_gt = origin_gt/255.0\n",
        "                        # Expand dims\n",
        "                    origin_image = np.expand_dims(origin_image, axis=0).transpose(0, 3, 1, 2).astype('uint8')\n",
        "                    origin_gt = np.expand_dims(origin_gt, axis=0).transpose(0, 3, 1, 2).astype('uint8')\n",
        "                        # ToTensor\n",
        "                    origin_image = torch.Tensor(origin_image).to(DEVICE)\n",
        "                    origin_gt = torch.Tensor(origin_gt).to(DEVICE)\n",
        "\n",
        "                    # Forward\n",
        "                    output = net(origin_image)\n",
        "\n",
        "                    # Backward\n",
        "                    opt.zero_grad()\n",
        "\n",
        "                    loss = criterion(output, origin_gt)\n",
        "                    loss.requires_grad_(True)\n",
        "                    loss.backward()\n",
        "\n",
        "                    opt.step()\n",
        "\n",
        "                    loss_arr += [loss.item()]\n",
        "                    \n",
        "                    break\n",
        "\n",
        "                # Concat output\n",
        "                sampling_outputs = []\n",
        "                for step, ds_image in enumerate(sampling_wound):\n",
        "                    # Preprocess\n",
        "                        # Norm input\n",
        "                    ds_image = ds_image/255.0\n",
        "                        # Expand dims\n",
        "                    ds_image = np.expand_dims(ds_image, axis=0).transpose(0, 3, 1, 2)\n",
        "                        # ToTensor\n",
        "                    ds_image = torch.Tensor(ds_image).to(DEVICE)\n",
        "\n",
        "                    # Forward\n",
        "                    ds_output = net(ds_image)\n",
        "                    sampling_outputs.append(ds_output)\n",
        "\n",
        "                # Postprocess\n",
        "                    # Norm ground-truth\n",
        "                target_gt = target_gt/255.0\n",
        "                    # Concat outputs\n",
        "                output = concat_gt_samples(sampling_outputs, anchor_axis, strides)\n",
        "                    # Expand dims\n",
        "                output = np.expand_dims(output, axis=0).transpose(1, 2, 0).astype('uint8')\n",
        "                target_gt = np.expand_dims(target_gt, axis=0).transpose(1, 2, 0).astype('uint8')\n",
        "                    # ToTensor\n",
        "                output = torch.Tensor(output).to(DEVICE)\n",
        "                target_gt = torch.Tensor(target_gt).to(DEVICE)\n",
        "\n",
        "                # Backward\n",
        "                opt.zero_grad()\n",
        "\n",
        "                loss = criterion(output, target_gt)\n",
        "                loss.requires_grad_(True)\n",
        "                loss.backward()\n",
        "\n",
        "                opt.step()\n",
        "\n",
        "                loss_arr += [loss.item()]\n",
        "\n",
        "            # Save loss\n",
        "            loss_arr += [loss.item()]\n",
        "            print(\"=======================================================\")\n",
        "            print(f\"VALID || EPOCH {epoch :04d} | BATCH {batch : 04d} / {math.ceil(num_batch_val) : 04d} | DICE LOSS {np.mean(loss_arr) : .4f}\")\n",
        "            print(\"=======================================================\")\n",
        "\n",
        "            # Save best loss\n",
        "            if batch > math.ceil((len(val_dataset) / BATCH_SIZE) / 2) :\n",
        "                if best_val_loss == -1 or np.mean(loss_arr) < best_val_loss:\n",
        "                    best_val_loss = np.mean(loss_arr)\n",
        "\n",
        "            # Save tensorboard\n",
        "            gt = fn_tonumpy(gt)\n",
        "            wound = fn_tonumpy(fn_denorm(wound, mean=0.5, std=0.5))\n",
        "            if len(output.shape) == 3:\n",
        "                output = output.to('cpu').detach().numpy()\n",
        "                output = np.expand_dims(output, axis=0)\n",
        "                output = fn_class(output)\n",
        "            else:\n",
        "                output = fn_tonumpy(fn_class(output))\n",
        "\n",
        "            writer_val.add_image('ground-truth', gt, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
        "            writer_val.add_image('wound', wound, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
        "            writer_val.add_image('output', output, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
        "\n",
        "        writer_val.add_scalar('loss', np.mean(loss_arr), epoch)\n",
        "        print(\"Write train loss, epoch \", epoch)\n",
        "        print()\n",
        "\n",
        "        # Early stopping\n",
        "        if before_val_loss != 0 and np.mean(loss_arr) > before_val_loss:\n",
        "            stop_cnt += 1\n",
        "\n",
        "            if stop_cnt >= 10:\n",
        "                is_early_stop = True\n",
        "        else:\n",
        "            stop_cnt = 0\n",
        "\n",
        "        before_val_loss = np.mean(loss_arr)\n",
        "\n",
        "    # Save network at epoch end\n",
        "    #save(ckpt_dir, net, opt, epoch)\n",
        "    print(\"ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\")\n",
        "\n",
        "    if is_early_stop: \n",
        "        print(\"Early stopping at epoch = \", epoch)\n",
        "        break\n",
        "\n",
        "# End write\n",
        "writer_train.close()\n",
        "writer_val.close()\n",
        "\n",
        "# Print best loss\n",
        "print(\"Best loss in Train = \", best_train_loss)\n",
        "print(\"Best loss in Validation = \", best_val_loss)\n",
        "\n",
        "# Print end time\n",
        "print(\"\\nModel train / validate time = \", time.time() - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3K6fDHMoFvO"
      },
      "source": [
        "### 4-4) Show train/val result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "peSs_UQlDphx"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp0bTovcDKq_"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir {MODEL_PATH + LOG_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6hYmUOcDLCh"
      },
      "source": [
        "### 4-5) Test benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OHrQHD0qDPH5"
      },
      "outputs": [],
      "source": [
        "# Set test dataset\n",
        "test_dataset = WoundDataset(data_dir=MODEL_PATH + DATASET_PATH + TEST_PATH, transform=trans)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "num_batch_test = len(test_dataset) / BATCH_SIZE\n",
        "\n",
        "# Set loss\n",
        "test_criterion = DiceLoss().to(DEVICE)\n",
        "\n",
        "# Set SummaryWriter\n",
        "if not os.path.exists(MODEL_PATH + LOG_PATH + TEST_PATH):\n",
        "    os.makedirs(MODEL_PATH + LOG_PATH + TEST_PATH)\n",
        "\n",
        "writer_test = SummaryWriter(log_dir=MODEL_PATH + LOG_PATH + TEST_PATH)\n",
        "\n",
        "# Test\n",
        "with torch.no_grad():\n",
        "    net.eval()\n",
        "    loss_arr = []\n",
        "\n",
        "    for batch, data in enumerate(val_loader, 1):\n",
        "        # Forward\n",
        "        scar = data['scar'].to(DEVICE)\n",
        "        label = data['label'].to(DEVICE)\n",
        "        output = net(scar)\n",
        "\n",
        "        # Calc loss\n",
        "        loss = test_criterion(output, label)\n",
        "\n",
        "        # Save loss\n",
        "        loss_arr += [loss.item()]\n",
        "        print(\"=======================================================\")\n",
        "        print(f\"TEST || BATCH {batch : 04d} / {math.ceil(len(test_dataset)/BATCH_SIZE) : 04d} | DICE_LOSS {np.mean(loss_arr) : .4f}\")\n",
        "        print(\"=======================================================\")\n",
        "\n",
        "        # Save tensorboard\n",
        "        label = fn_tonumpy(label)\n",
        "        scar = fn_tonumpy(fn_denorm(scar, mean=0.5, std=0.5))\n",
        "        output = fn_tonumpy(fn_class(output))\n",
        "\n",
        "        writer_test.add_image('label', label, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
        "        writer_test.add_image('scar', scar, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
        "        writer_test.add_image('output', output, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
        "\n",
        "    writer_test.add_scalar('loss', np.mean(loss_arr), epoch)\n",
        "    print(\"ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZYSI2VPOMYV"
      },
      "source": [
        "### 4-6) Show test result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T7AOpup2ON6B"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir {MODEL_PATH + LOG_PATH}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "xuju0_8jyNnP",
        "-ERI7fzVOrkQ"
      ],
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNr+0ESBF8xs4LfJG+S5BIg",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}