{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RawDataset_Processor-Scar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXVLIkQTtItWU2mC3Uvknk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaeSeokSong/image-processing/blob/feature%2FUnet-scar/RawDataset_Processor_Scar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RawDataset_Processor 실행 후 UNet 실행"
      ],
      "metadata": {
        "id": "bfXmf9amyDsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "j_SoWfFhxL0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image processing\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import output\n",
        "\n",
        "# ETC\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "xpg4AUPixNrD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount google drive"
      ],
      "metadata": {
        "id": "8OHL2n1YxlH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxOqA3DpxlgC",
        "outputId": "8bec7833-abd7-497b-fd78-eb0ecf441aa5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/Models/GAN_Scar\n",
        "!ls -al"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqwTBfCyxm3v",
        "outputId": "69bfcaf7-bf15-4a8e-c579-62d4c2a430d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Models/GAN_Scar\n",
            "total 176\n",
            "drwx------ 2 root root  4096 Aug 17 05:23  Dataset\n",
            "-rw------- 1 root root 86402 Aug 13 09:16  Image_segmentation-Scar.ipynb\n",
            "drwx------ 4 root root  4096 Aug 17 05:49  Log\n",
            "drwx------ 4 root root  4096 Aug 16 08:39  Raw_Dataset\n",
            "-rw------- 1 root root  8311 Aug 17 06:42  RawDataset_Processor-Scar.ipynb\n",
            "-rw------- 1 root root 39995 Aug 15 11:40 'UNet architecture.PNG'\n",
            "-rw------- 1 root root 31549 Aug 17 06:38  Unet-Scar.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grobal variable"
      ],
      "metadata": {
        "id": "QoO0sORTxEIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path\n",
        "MODEL_PATH = \"/content/gdrive/MyDrive/Models/GAN_Scar\"\n",
        "\n",
        "RAW_TRAIN_SET_PATH = \"/Raw_Dataset/train\"\n",
        "RAW_TEST_SET_PATH = \"/Raw_Dataset/test\"\n",
        "\n",
        "DATASET_PATH = \"/Dataset\"\n",
        "\n",
        "IMAGES_PATH = '/images'\n",
        "LABELS_PATH = '/labels'\n",
        "\n",
        "# Image preprocess\n",
        "NORM_INPUT_W_SIZE = 320\n",
        "NORM_INPUT_H_SIZE = 240"
      ],
      "metadata": {
        "id": "Z9c_qcr4xHQR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function"
      ],
      "metadata": {
        "id": "qnPV28oKmmrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_image(image):\n",
        "    # Resize image\n",
        "    imageW = image.shape[0]\n",
        "    imageH = image.shape[1]\n",
        "\n",
        "    resizeRatioW = imageW / NORM_INPUT_W_SIZE\n",
        "    resizeRatioH = imageH / NORM_INPUT_H_SIZE\n",
        "\n",
        "    image = cv2.resize(image, \n",
        "                       (int(imageW / resizeRatioW), int(imageH / resizeRatioH)), \n",
        "                       interpolation=cv2.INTER_CUBIC\n",
        "                       )\n",
        "    \n",
        "    return image"
      ],
      "metadata": {
        "id": "_M2PWIKjmo0X"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "stLwAdly0yap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make dir"
      ],
      "metadata": {
        "id": "GNFISZzoM4Fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create processed dataset dir\n",
        "train_dir = os.path.join(MODEL_PATH + DATASET_PATH, 'train')\n",
        "val_dir = os.path.join(MODEL_PATH + DATASET_PATH, 'val')\n",
        "test_dir = os.path.join(MODEL_PATH + DATASET_PATH, 'test')\n",
        "\n",
        "if not os.path.exists(train_dir):\n",
        "    os.makedirs(train_dir)\n",
        "\n",
        "if not os.path.exists(val_dir):\n",
        "    os.makedirs(val_dir)\n",
        "\n",
        "if not os.path.exists(test_dir):\n",
        "    os.makedirs(test_dir)"
      ],
      "metadata": {
        "id": "3m21Fyyv29tc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process train dataset"
      ],
      "metadata": {
        "id": "bxVhdvBq20nW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "os.chdir(image_path)\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Divide train:val = 7:3\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.7)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Get HSV image\n",
        "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    hue_image, sat_image, val_image = cv2.split(hsv_image)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(val_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "    val_image = val_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Resize image\n",
        "    val_image = resize_image(val_image)\n",
        "\n",
        "    # Save normalized image\n",
        "    # val_image는 흑백 이미지라 1채널에 각 픽셀값들도 0~255로 정규화 되어있다.\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), val_image)\n",
        "    #np.save(os.path.join(train_dir, f'scar_{idx:03d}.npy'), val_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Get HSV image\n",
        "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    hue_image, sat_image, val_image = cv2.split(hsv_image)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(val_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "    val_image = val_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Resize image\n",
        "    val_image = resize_image(val_image)\n",
        "\n",
        "    # Save normalized image\n",
        "    # val_image는 흑백 이미지라 1채널에 각 픽셀값들도 0~255로 정규화 되어있다.\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{val_idx:03d}.png'), val_image)\n",
        "    #np.save(os.path.join(val_dir, f'scar_{val_idx:03d}.npy'), val_image)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load raw label images\n",
        "os.chdir(label_path)\n",
        "label_files = os.listdir(label_path)\n",
        "label_files.sort()\n",
        "\n",
        "# Create processed scar label for train\n",
        "for idx in range(train_size):\n",
        "    label_image = cv2.imread(label_files[idx], cv2.IMREAD_GRAYSCALE)\n",
        "    \n",
        "    # Resize image\n",
        "    label_image = resize_image(label_image)\n",
        "\n",
        "    # Save normalized image\n",
        "    # 그레이 스케일은 흑백으로, 1채널에 각 픽셀값들도 0~255로 정규화 되어있다.\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "    #np.save(os.path.join(train_dir, f'label_{idx:03d}.npy'), label_image)\n",
        "\n",
        "# Create processed scar label for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    label_image = cv2.imread(label_files[idx], cv2.IMREAD_GRAYSCALE)\n",
        "    \n",
        "    # Resize image\n",
        "    label_image = resize_image(label_image)\n",
        "\n",
        "    # Save normalized image\n",
        "    # 그레이 스케일은 흑백으로, 1채널에 각 픽셀값들도 0~255로 정규화 되어있다.\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{val_idx:03d}.png'), label_image)\n",
        "    #np.save(os.path.join(val_dir, f'label_{val_idx:03d}.npy'), label_image)"
      ],
      "metadata": {
        "id": "yljPDLtm0zuo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process test dataset"
      ],
      "metadata": {
        "id": "AU5G6MgG25A0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = MODEL_PATH + RAW_TEST_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TEST_SET_PATH + LABELS_PATH\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load raw train images\n",
        "os.chdir(image_path)\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(len(train_files)): \n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Get HSV image\n",
        "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    hue_image, sat_image, val_image = cv2.split(hsv_image)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(val_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "    val_image = val_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Resize image\n",
        "    val_image = resize_image(val_image)\n",
        "\n",
        "    # Save normalized image\n",
        "    # val_image는 흑백 이미지라 1채널에 각 픽셀값들도 0~255로 정규화 되어있다.\n",
        "    cv2.imwrite(os.path.join(test_dir, f'scar_{idx:03d}.png'), val_image)\n",
        "    #np.save(os.path.join(test_dir, f'scar_{idx:03d}.npy'), val_image)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load raw label images\n",
        "os.chdir(label_path)\n",
        "label_files = os.listdir(label_path)\n",
        "label_files.sort()\n",
        "\n",
        "# Create processed scar label for train\n",
        "for idx in range(len(label_files)):\n",
        "    label_image = cv2.imread(label_files[idx], cv2.IMREAD_GRAYSCALE)\n",
        "    \n",
        "    # Resize image\n",
        "    label_image = resize_image(label_image)\n",
        "\n",
        "    # Save normalized image\n",
        "    # 그레이 스케일은 흑백으로, 1채널에 각 픽셀값들도 0~255로 정규화 되어있다.\n",
        "    cv2.imwrite(os.path.join(test_dir, f'label_{idx:03d}.png'), label_image)\n",
        "    #np.save(os.path.join(test_dir, f'label_{idx:03d}.npy'), label_image)"
      ],
      "metadata": {
        "id": "hIU5Cv4f2wGo"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}