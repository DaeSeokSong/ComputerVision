{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unet-Scar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP26TOT28hOXevz0ErEcw5G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaeSeokSong/image-processing/blob/feature%2FUnet-scar/Unet_Scar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ※ Precautions\n",
        "\n",
        "1.   RawDataset_Processor 실행 후 UNet 실행 추가\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xuju0_8jyNnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 『Reference』\n",
        "\n",
        "* *Paper*\n",
        ">   [U-net](https://paperswithcode.com/paper/u-net-convolutional-networks-for-biomedical)\n",
        ">\n",
        "> 기존 CNN은 Single classification task에 사용되었지만,\n",
        "> \n",
        "> biomedical image processing 분야에서는 한 이미지 내의 모든 pixel을 classification 하는 Semantic segmentation task가 중요하게 사용되었다.\n",
        ">\n",
        ">  sliding window 방식을 사용하는 CNN 구조와 달리 검증된 patch는 넘기기 때문에 보다 빠른 처리가 가능한 구조이다.\n",
        "> \n",
        "> 적은 양의 데이터로도 dataset argumentation을 통해 잘 학습시킬 수 있다.\n",
        ">\n",
        ">   * [U-net++](https://paperswithcode.com/paper/unet-a-nested-u-net-architecture-for-medical)\n",
        ">   * [ResUNet++](https://paperswithcode.com/paper/resunet-an-advanced-architecture-for-medical)\n",
        "\n",
        "<br>\n",
        "\n",
        "* *Lecture*\n",
        "> * [UNet architecture by pytorch](https://89douner.tistory.com/300)\n",
        "\n"
      ],
      "metadata": {
        "id": "ETKXZy_BpRBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Development enviroment"
      ],
      "metadata": {
        "id": "lgbE_AiY8yV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Import"
      ],
      "metadata": {
        "id": "NVNq8Y-A89nl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1) Library"
      ],
      "metadata": {
        "id": "_vKiIgjo9Dro"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZsXjyWownhef"
      },
      "outputs": [],
      "source": [
        "# U-net\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "# Image processing\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pickle as pl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import output\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "# ETC\n",
        "import os\n",
        "import time\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2) Mount google drive"
      ],
      "metadata": {
        "id": "Pcj_F7KZ9FEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "I1I8TPjO9INs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "439d78ce-d065-4cb3-b980-a1be4844dca3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/Models/GAN_Scar\n",
        "!ls -al"
      ],
      "metadata": {
        "id": "QrRWaQd09Joq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4634b4d2-3ad3-4e2f-a2ab-e04001f17c2b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Models/GAN_Scar\n",
            "total 193\n",
            "drwx------ 4 root root  4096 Aug 16 08:53  Dataset\n",
            "-rw------- 1 root root 86402 Aug 13 09:16  Image_segmentation-Scar.ipynb\n",
            "drwx------ 4 root root  4096 Aug 27 09:42  Log\n",
            "-rw------- 1 root root 21476 Aug 27 09:31  Processor_PerformanceTester-Scar.ipynb\n",
            "drwx------ 2 root root  4096 Aug 16 08:39  Raw_Dataset\n",
            "-rw------- 1 root root   391 Aug 24 11:04  RawDataset_Processor-Scar.ipynb\n",
            "drwx------ 2 root root  4096 Aug 23 14:47  result\n",
            "-rw------- 1 root root 39995 Aug 15 11:40 'UNet architecture.PNG'\n",
            "-rw------- 1 root root 32199 Aug 27 09:45  Unet-Scar.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Train U-Net"
      ],
      "metadata": {
        "id": "UGNH-Msw9Lyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Grobal variable"
      ],
      "metadata": {
        "id": "6V4ug77i9eo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path\n",
        "MODEL_PATH = \"/content/gdrive/MyDrive/Models/GAN_Scar\"\n",
        "\n",
        "DATASET_PATH = \"/Dataset\"\n",
        "LOG_PATH = \"/Log\"\n",
        "\n",
        "TRAIN_PATH = \"/train\"\n",
        "VAL_PATH = \"/val\"\n",
        "TEST_PATH = \"/test\"\n",
        "\n",
        "# Train hyperparameter\n",
        "LR = 1e-3\n",
        "BATCH_SIZE = 8 # batch size 8 초과부터는 Colab gpu ram 용량 초과로 원활한 학습 불가\n",
        "EPOCHS = 20\n",
        "\n",
        "\"\"\"\n",
        "GPU 사용이 가능하면 cuda 사용\n",
        "아니면 CPU를 이용하여 학습\n",
        "\"\"\"\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "IfyygS-p9gzc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Funtion"
      ],
      "metadata": {
        "id": "EtN2YVKdzEo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-1) Convenience func"
      ],
      "metadata": {
        "id": "XrmqoZrZzGz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow_waitkey_enter(image):\n",
        "    cv2_imshow(image)\n",
        "\n",
        "    time.sleep(0.5)\n",
        "    \n",
        "    input(\"Please press the Enter key to proceed\\n\")\n",
        "    output.clear()\n",
        "\n",
        "    pass"
      ],
      "metadata": {
        "id": "x7qxDCQnzGcm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Class"
      ],
      "metadata": {
        "id": "Vin2caAL9b0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-1) Custom U-Net"
      ],
      "metadata": {
        "id": "l7ttKi5fU8L6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3-1-1) Architecture"
      ],
      "metadata": {
        "id": "zj6HFu6eUeuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**마지막 Output 채널 2 → 1 변경**\n",
        "> Binary classification\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=14CzAAaKv5v7pVfvugBRbD1xI4IuhmoyT\"  width = 640>"
      ],
      "metadata": {
        "id": "pYEyOHCoTsJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3-1-2) Build network"
      ],
      "metadata": {
        "id": "-ERI7fzVOrkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.nn의 Module 클래스를 상속한, 커스텀 UNet 클래스\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # kernel size, stride, padding, bias는 거의 고정 >> predefine\n",
        "        def ConvBatchReLU_2d(in_ch, out_ch, k_size=3, stride=1, padding=0, bias=True):\n",
        "            layers = []\n",
        "\n",
        "            # Add Conv layer\n",
        "            layers += [nn.Conv2d(in_channels=in_ch,\n",
        "                                 out_channels=out_ch,\n",
        "                                 kernel_size=k_size,\n",
        "                                 stride=stride,\n",
        "                                 padding=padding,\n",
        "                                 bias=bias\n",
        "                                 )]\n",
        "\n",
        "            # Add batch normalization layer\n",
        "            layers += [nn.BatchNorm2d(num_features=out_ch)]\n",
        "\n",
        "            # Add ReLU\n",
        "            layers += [nn.ReLU()]\n",
        "\n",
        "            # Define conv, ReLU step in contracting path\n",
        "            CBR = nn.Sequential(*layers)\n",
        "\n",
        "            return CBR\n",
        "\n",
        "        \"\"\"\n",
        "        [Contracting path]\n",
        "        >> 입력 이미지의 context 포착이 목적\n",
        "        \"\"\"\n",
        "        # enc == encoder / n_m == n번째 stage(step)의 m번째 레이어\n",
        "        self.enc1_1 = ConvBatchReLU_2d(in_ch=1, out_ch=64)\n",
        "        self.enc1_2 = ConvBatchReLU_2d(in_ch=64, out_ch=64)\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc2_1 = ConvBatchReLU_2d(in_ch=64, out_ch=128)\n",
        "        self.enc2_2 = ConvBatchReLU_2d(in_ch=128, out_ch=128)\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc3_1 = ConvBatchReLU_2d(in_ch=128, out_ch=256)\n",
        "        self.enc3_2 = ConvBatchReLU_2d(in_ch=256, out_ch=256)\n",
        "\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc4_1 = ConvBatchReLU_2d(in_ch=256, out_ch=512)\n",
        "        self.enc4_2 = ConvBatchReLU_2d(in_ch=512, out_ch=512)\n",
        "\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc5_1 = ConvBatchReLU_2d(in_ch=512, out_ch=1024)\n",
        "\n",
        "        \"\"\"\n",
        "        [Expansive path]\n",
        "        >> 세밀한 Localization을 위한 높은 차원의 채널을 갖는 Upsampling\n",
        "        >> 얕은 레이어의 특집 맵을 결합\n",
        "        \"\"\"\n",
        "        # dec == decoder\n",
        "        self.dec5_1 = ConvBatchReLU_2d(in_ch=1024, out_ch=512)\n",
        "\n",
        "        # up-conv 레이어는 채널을 복원을 해야하기 때문에 kernel size를\n",
        "        # 대칭되는 MaxPool layer의 kernel size와 같도록 설정한다.\n",
        "        self.unpool4 = nn.ConvTranspose2d(in_channels=512,\n",
        "                                          out_channels=512,\n",
        "                                          kernel_size=2,\n",
        "                                          stride=2,\n",
        "                                          padding=0,\n",
        "                                          bias=True)\n",
        "        \n",
        "        # input channel은 up-conv와 대칭되는 enc, 두 레이어에서\n",
        "        # 같은 크기의 채널로 오기 때문에 대칭 enc 레이어보다 input이 두 배 많다.\n",
        "        self.dec4_2 = ConvBatchReLU_2d(in_ch=2 * 512, out_ch=512)\n",
        "        self.dec4_1 = ConvBatchReLU_2d(in_ch=512, out_ch=256)\n",
        "\n",
        "        self.unpool3 = nn.ConvTranspose2d(in_channels=256,\n",
        "                                          out_channels=256,\n",
        "                                          kernel_size=2,\n",
        "                                          stride=2,\n",
        "                                          padding=0,\n",
        "                                          bias=True)\n",
        "        \n",
        "        self.dec3_2 = ConvBatchReLU_2d(in_ch=2 * 256, out_ch=256)\n",
        "        self.dec3_1 = ConvBatchReLU_2d(in_ch=256, out_ch=128)\n",
        "\n",
        "        self.unpool2 = nn.ConvTranspose2d(in_channels=128,\n",
        "                                          out_channels=128,\n",
        "                                          kernel_size=2,\n",
        "                                          stride=2,\n",
        "                                          padding=0,\n",
        "                                          bias=True)\n",
        "        \n",
        "        self.dec2_2 = ConvBatchReLU_2d(in_ch=2 * 128, out_ch=128)\n",
        "        self.dec2_1 = ConvBatchReLU_2d(in_ch=128, out_ch=64)\n",
        "\n",
        "        self.unpool1 = nn.ConvTranspose2d(in_channels=64,\n",
        "                                          out_channels=64,\n",
        "                                          kernel_size=2,\n",
        "                                          stride=2,\n",
        "                                          padding=0,\n",
        "                                          bias=True)\n",
        "        \n",
        "        self.dec1_2 = ConvBatchReLU_2d(in_ch=2 * 64, out_ch=64)\n",
        "        self.dec1_1 = ConvBatchReLU_2d(in_ch=64, out_ch=64)\n",
        "\n",
        "        # conv 1*1, N class for segmentation\n",
        "        # 이미지 상에서는 out_channels 2라 되어있으나 결과 도출을 위해 1로 설정\n",
        "        self.conv = nn.Conv2d(in_channels=64,\n",
        "                              out_channels=1,\n",
        "                              kernel_size=1,\n",
        "                              stride=1,\n",
        "                              padding=0,\n",
        "                              bias=True)\n",
        "        \n",
        "    # x == input_image\n",
        "    def forward(self, x):\n",
        "        print(\"Input size = \", x.shape)\n",
        "        input_width, input_height = x.shape[2], x.shape[3]\n",
        "\n",
        "        # encoder part\n",
        "        enc1_1 = self.enc1_1(x)\n",
        "        enc1_2 = self.enc1_2(enc1_1)\n",
        "        pool1 = self.pool1(enc1_2)\n",
        "\n",
        "        enc2_1 = self.enc2_1(pool1)\n",
        "        enc2_2 = self.enc2_2(enc2_1)\n",
        "        pool2 = self.pool2(enc2_2)\n",
        "        \n",
        "        enc3_1 = self.enc3_1(pool2)\n",
        "        enc3_2 = self.enc3_2(enc3_1)\n",
        "        pool3 = self.pool3(enc3_2)\n",
        "\n",
        "        enc4_1 = self.enc4_1(pool3)\n",
        "        enc4_2 = self.enc4_2(enc4_1)\n",
        "        pool4 = self.pool4(enc4_2)\n",
        "\n",
        "        enc5_1 = self.enc5_1(pool4)\n",
        "\n",
        "        # decoder part\n",
        "        dec5_1 = self.dec5_1(enc5_1)\n",
        "\n",
        "        unpool4 = self.unpool4(dec5_1)\n",
        "\n",
        "        \"\"\"\n",
        "        [Skip connection]\n",
        "        >> Semantic segmentation에서는 위치정보가 중요하기에\n",
        "        >> 이에 대한 소실 방지 차원에서 이전 연산했던 값을 더해준다.\n",
        "\n",
        "        >> copy and crop\n",
        "        >> encoding 데이터가 더 크므로 복사(copy)후 잘라준다(crop)\n",
        "        \"\"\"\n",
        "\n",
        "        # copy and crop\n",
        "        cpy_enc4_2 =  enc4_2.clone().cpu().detach().numpy()\n",
        "        diff_y = cpy_enc4_2.shape[2] - unpool4.shape[2]\n",
        "        diff_x = cpy_enc4_2.shape[3] - unpool4.shape[3]\n",
        "\n",
        "        top = int(diff_y / 2)\n",
        "        left = int(diff_x / 2)\n",
        "        height = int(cpy_enc4_2.shape[2] - top)\n",
        "        width = int(cpy_enc4_2.shape[3] - left)\n",
        "\n",
        "        cpy_enc4_2 = cpy_enc4_2[:, :, top:height, left:width]\n",
        "        cpy_enc4_2 = torch.Tensor(cpy_enc4_2).to(DEVICE)\n",
        "\n",
        "        cat4 = torch.cat((unpool4, cpy_enc4_2), dim=1)\n",
        "        dec4_2 = self.dec4_2(cat4)\n",
        "        dec4_1 = self.dec4_1(dec4_2)\n",
        "\n",
        "        unpool3 = self.unpool3(dec4_1)\n",
        "\n",
        "        # copy and crop\n",
        "        cpy_enc3_2 =  enc3_2.clone().cpu().detach().numpy()\n",
        "        diff_y = cpy_enc3_2.shape[2] - unpool3.shape[2]\n",
        "        diff_x = cpy_enc3_2.shape[3] - unpool3.shape[3]\n",
        "\n",
        "        top = int(diff_y / 2)\n",
        "        left = int(diff_x / 2)\n",
        "        height = int(cpy_enc3_2.shape[2] - top)\n",
        "        width = int(cpy_enc3_2.shape[3] - left)\n",
        "\n",
        "        cpy_enc3_2 = cpy_enc3_2[:, :, top:height, left:width]\n",
        "        cpy_enc3_2 = torch.Tensor(cpy_enc3_2).to(DEVICE)\n",
        "\n",
        "        cat3 = torch.cat((unpool3, cpy_enc3_2), dim=1)\n",
        "        dec3_2 = self.dec3_2(cat3)\n",
        "        dec3_1 = self.dec3_1(dec3_2)\n",
        "\n",
        "        unpool2 = self.unpool2(dec3_1)\n",
        "\n",
        "        # copy and crop\n",
        "        cpy_enc2_2 =  enc2_2.clone().cpu().detach().numpy()\n",
        "        diff_y = cpy_enc2_2.shape[2] - unpool2.shape[2]\n",
        "        diff_x = cpy_enc2_2.shape[3] - unpool2.shape[3]\n",
        "\n",
        "        top = int(diff_y / 2)\n",
        "        left = int(diff_x / 2)\n",
        "        height = int(cpy_enc2_2.shape[2] - top)\n",
        "        width = int(cpy_enc2_2.shape[3] - left)\n",
        "\n",
        "        cpy_enc2_2 = cpy_enc2_2[:, :, top:height, left:width]\n",
        "        cpy_enc2_2 = torch.Tensor(cpy_enc2_2).to(DEVICE)\n",
        "\n",
        "        cat2 = torch.cat((unpool2, cpy_enc2_2), dim=1)\n",
        "        dec2_2 = self.dec2_2(cat2)\n",
        "        dec2_1 = self.dec2_1(dec2_2)\n",
        "\n",
        "        unpool1 = self.unpool1(dec2_1)\n",
        "\n",
        "        # copy and crop\n",
        "        cpy_enc1_2 =  enc1_2.clone().cpu().detach().numpy()\n",
        "        diff_y = cpy_enc1_2.shape[2] - unpool1.shape[2]\n",
        "        diff_x = cpy_enc1_2.shape[3] - unpool1.shape[3]\n",
        "\n",
        "        top = int(diff_y / 2)\n",
        "        left = int(diff_x / 2)\n",
        "        height = int(cpy_enc1_2.shape[2] - top)\n",
        "        width = int(cpy_enc1_2.shape[3] - left)\n",
        "\n",
        "        cpy_enc1_2 = cpy_enc1_2[:, :, top:height, left:width]\n",
        "        cpy_enc1_2 = torch.Tensor(cpy_enc1_2).to(DEVICE)\n",
        "\n",
        "        cat1 = torch.cat((unpool1, cpy_enc1_2), dim=1)\n",
        "        dec1_2 = self.dec1_2(cat1)\n",
        "        dec1_1 = self.dec1_1(dec1_2)\n",
        "\n",
        "        x = self.conv(dec1_1)\n",
        "        print(\"Output size = \", x.shape)\n",
        "        output_width, output_height = x.shape[2], x.shape[3]\n",
        "\n",
        "        \"\"\"\n",
        "        [mirroring extrapolation]\n",
        "\n",
        "        input(572*572)/output(388*388) 사이즈가 다르므로 output에 \n",
        "        mirroring extrapolation 기법으로 missing context 부분을 채운다.\n",
        "        \"\"\"\n",
        "        pad_width = int((input_width - output_width) / 2)\n",
        "        pad_height = int((input_height - output_height) / 2)\n",
        "        extrapolation = []\n",
        "        for image in x:\n",
        "            #image = nn.functional.pad(image, (pad_width, pad_width, pad_height, pad_height), 'reflect') # 이미지, (좌, 우, 상, 하), 방법\n",
        "            image = nn.functional.pad(image, (pad_width, pad_width, pad_height, pad_height), 'constant', 0)\n",
        "            extrapolation.append(image)\n",
        "\n",
        "        output = torch.stack(extrapolation, dim=0).to(DEVICE)\n",
        "        #print(\"Mirrored padding output size = \", output.shape)\n",
        "        print(\"Constant padding output size = \", output.shape)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "JUWtaQKf9dJ0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-2) Pytorch"
      ],
      "metadata": {
        "id": "wxCDqnkVU_fm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3-2-1) Dataset"
      ],
      "metadata": {
        "id": "xkgZx5qbVJNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScarDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        lst_data = os.listdir(self.data_dir)\n",
        "\n",
        "        lst_scar = [f for f in lst_data if f.startswith('scar')]\n",
        "        lst_label = [f for f in lst_data if f.startswith('label')]\n",
        "\n",
        "        lst_scar.sort()\n",
        "        lst_label.sort()\n",
        "\n",
        "        self.lst_scar = lst_scar\n",
        "        self.lst_label = lst_label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lst_label)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        scar = cv2.imread(os.path.join(self.data_dir, self.lst_scar[index]),\n",
        "                          cv2.IMREAD_GRAYSCALE)\n",
        "        label = cv2.imread(os.path.join(self.data_dir, self.lst_label[index]),\n",
        "                          cv2.IMREAD_GRAYSCALE)\n",
        "        #scar = np.load(os.path.join(self.data_dir, self.lst_scar[index]))\n",
        "        #label = np.load(os.path.join(self.data_dir, self.lst_label[index]))\n",
        "\n",
        "        scar = scar/255.0\n",
        "        label = label/255.0\n",
        "\n",
        "        if scar.ndim == 2:\n",
        "            scar = scar[:, :, np.newaxis]\n",
        "        if label.ndim == 2:\n",
        "            label = label[:, :, np.newaxis]\n",
        "\n",
        "        data = {'scar': scar, 'label': label}\n",
        "\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "\n",
        "        return data"
      ],
      "metadata": {
        "id": "VqnUx025VKpg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3-2-2) Transform"
      ],
      "metadata": {
        "id": "3Rv22BEuok0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ToTensor(object):\n",
        "    def __call__(self, data):\n",
        "        scar, label = data['scar'], data['label']\n",
        "        scar = scar.transpose((2, 0, 1)).astype(np.float32)\n",
        "        label = label.transpose((2, 0, 1)).astype(np.float32)\n",
        "\n",
        "        data = {'scar': torch.from_numpy(scar), 'label': torch.from_numpy(label)}\n",
        "\n",
        "        return data\n",
        "\n",
        "# Dataset arugmentation\n",
        "class Normalization(object):\n",
        "    def __init__(self, mean=0.5, std=0.5):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, data):\n",
        "        scar, label = data['scar'], data['label']\n",
        "\n",
        "        scar = (scar - self.mean) / self.std\n",
        "\n",
        "        data = {'scar': scar, 'label': label}\n",
        "\n",
        "        return data\n",
        "\n",
        "class RandomFlip(object):\n",
        "    def __call__(self, data):\n",
        "        scar, label = data['scar'], data['label']\n",
        "\n",
        "        if np.random.rand() > 0.5:\n",
        "            scar = np.fliplr(scar)\n",
        "            label = np.fliplr(label)\n",
        "\n",
        "        if np.random.rand() > 0.5:\n",
        "            scar = np.flipud(scar)\n",
        "            label = np.flipud(label)\n",
        "\n",
        "        data = {'scar': scar, 'label': label}\n",
        "\n",
        "        return data"
      ],
      "metadata": {
        "id": "1FfoKGJZol9C"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Run"
      ],
      "metadata": {
        "id": "OsjfNZrh9Z8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-1) Prepare dataset"
      ],
      "metadata": {
        "id": "xu1q7hsx9p9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trans = transforms.Compose([Normalization(),\n",
        "                            RandomFlip(),\n",
        "                            ToTensor()])\n",
        "\n",
        "train_dataset = ScarDataset(data_dir=MODEL_PATH + DATASET_PATH + TRAIN_PATH, transform=trans)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "val_dataset = ScarDataset(data_dir=MODEL_PATH + DATASET_PATH + VAL_PATH, transform=trans)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "rxBFGyd89aps"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-2) Set variable about training"
      ],
      "metadata": {
        "id": "xbdBhwie-Q1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = UNet().to(DEVICE)\n",
        "criterion = nn.BCEWithLogitsLoss().to(DEVICE)\n",
        "\n",
        "# Set optimizer\n",
        "opt = torch.optim.Adam(net.parameters(), lr=LR)\n",
        "\n",
        "num_batch_train = len(train_dataset) / BATCH_SIZE\n",
        "num_batch_val = len(val_dataset) / BATCH_SIZE\n",
        "\n",
        "# Set lambda func\n",
        "fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n",
        "fn_denorm = lambda x, mean, std: (x * std) + mean\n",
        "fn_class = lambda x: 1.0 * (x > 0.5)\n",
        "\n",
        "# Set SummaryWriter\n",
        "if not os.path.exists(MODEL_PATH + LOG_PATH + TRAIN_PATH):\n",
        "    os.makedirs(MODEL_PATH + LOG_PATH + TRAIN_PATH)\n",
        "\n",
        "if not os.path.exists(MODEL_PATH + LOG_PATH + VAL_PATH):\n",
        "    os.makedirs(MODEL_PATH + LOG_PATH + VAL_PATH)\n",
        "\n",
        "writer_train = SummaryWriter(log_dir=MODEL_PATH + LOG_PATH + TRAIN_PATH)\n",
        "writer_val = SummaryWriter(log_dir=MODEL_PATH + LOG_PATH + VAL_PATH)"
      ],
      "metadata": {
        "id": "Jcr35-y8-Rk4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-3) Train model"
      ],
      "metadata": {
        "id": "sd82Nd77WOuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "\n",
        "for epoch in range(start_epoch+1, EPOCHS+1):\n",
        "    net.train()\n",
        "    loss_arr = []\n",
        "\n",
        "    for batch, data in enumerate(train_loader, 1):\n",
        "        # Forward\n",
        "        scar = data['scar'].to(DEVICE)\n",
        "        label = data['label'].to(DEVICE)\n",
        "        output = net(scar)\n",
        "\n",
        "        # Backward\n",
        "        opt.zero_grad()\n",
        "\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        # Save loss\n",
        "        loss_arr += [loss.item()]\n",
        "        print(\"==================================================\")\n",
        "        print(f\"TRAIN || EPOCH {epoch :04d} | BATCH {batch : 04d} / {math.ceil(len(train_dataset)/BATCH_SIZE) : 04d} | LOSS {np.mean(loss_arr) : .4f}\")\n",
        "        print(\"==================================================\")\n",
        "\n",
        "        # Save tensorboard\n",
        "        label = fn_tonumpy(label)\n",
        "        scar = fn_tonumpy(fn_denorm(scar, mean=0.5, std=0.5))\n",
        "        output = fn_tonumpy(fn_class(output))\n",
        "\n",
        "        writer_train.add_image('label', label, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
        "        writer_train.add_image('scar', scar, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
        "        writer_train.add_image('output', output, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
        "\n",
        "        print(\"ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\")\n",
        "\n",
        "    writer_train.add_scalar('loss', np.mean(loss_arr), epoch)\n",
        "\n",
        "    # Validate\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        loss_arr = []\n",
        "\n",
        "        for batch, data in enumerate(val_loader, 1):\n",
        "            # Forward\n",
        "            scar = data['scar'].to(DEVICE)\n",
        "            label = data['label'].to(DEVICE)\n",
        "            output = net(scar)\n",
        "\n",
        "            # Calc loss\n",
        "            loss = criterion(output, label)\n",
        "\n",
        "            # Save loss\n",
        "            loss_arr += [loss.item()]\n",
        "            print(\"==================================================\")\n",
        "            print(f\"VALID || EPOCH {epoch :04d} | BATCH {batch : 04d} / {math.ceil(len(val_dataset)/BATCH_SIZE) : 04d} | LOSS {np.mean(loss_arr) : .4f}\")\n",
        "            print(\"==================================================\")\n",
        "\n",
        "            # Save tensorboard\n",
        "            label = fn_tonumpy(label)\n",
        "            scar = fn_tonumpy(fn_denorm(scar, mean=0.5, std=0.5))\n",
        "            output = fn_tonumpy(fn_class(output))\n",
        "\n",
        "            writer_val.add_image('label', label, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
        "            writer_val.add_image('scar', scar, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
        "            writer_val.add_image('output', output, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
        "\n",
        "            print(\"ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\")\n",
        "\n",
        "        writer_val.add_scalar('loss', np.mean(loss_arr), epoch)\n",
        "\n",
        "    print(\"ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\")\n",
        "\n",
        "writer_train.close()\n",
        "writer_val.close()"
      ],
      "metadata": {
        "id": "WmmcOJqKWQzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-4) Show train result"
      ],
      "metadata": {
        "id": "F3K6fDHMoFvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "peSs_UQlDphx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir {MODEL_PATH + LOG_PATH}"
      ],
      "metadata": {
        "id": "Lp0bTovcDKq_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}