{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaeSeokSong/image-processing/blob/feature%2FUnet-scar/Processor_PerformanceTester_Scar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_SoWfFhxL0-"
      },
      "source": [
        "# **Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xpg4AUPixNrD"
      },
      "outputs": [],
      "source": [
        "# Image processing\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import output\n",
        "\n",
        "# ETC\n",
        "import os\n",
        "import time\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OHL2n1YxlH_"
      },
      "source": [
        "## Mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxOqA3DpxlgC",
        "outputId": "92dd177d-58da-413e-a324-539637f400d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqwTBfCyxm3v",
        "outputId": "eea1aa17-a055-41c7-b0f0-ef7e3099a3e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Models/GAN_Scar\n",
            "total 205\n",
            "drwx------ 2 root root  4096 Sep  8 13:49  Dataset\n",
            "-rw------- 1 root root 22891 Sep  1 12:04  Image_segmentation-Scar.ipynb\n",
            "drwx------ 3 root root  4096 Aug 17 05:49  Log\n",
            "-rw------- 1 root root 63460 Sep  8 14:05  Processor_PerformanceTester-Scar.ipynb\n",
            "drwx------ 4 root root  4096 Aug 16 08:39  Raw_Dataset\n",
            "-rw------- 1 root root 16167 Sep  1 12:13  RawDataset_Processor-Scar.ipynb\n",
            "drwx------ 2 root root  4096 Aug 23 14:47  result\n",
            "-rw------- 1 root root 39995 Aug 15 11:40 'UNet architecture.PNG'\n",
            "-rw------- 1 root root 49822 Sep  8 14:00  Unet-Scar.ipynb\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/MyDrive/Models/GAN_Scar\n",
        "!ls -al"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoO0sORTxEIQ"
      },
      "source": [
        "# **Grobal variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Z9c_qcr4xHQR"
      },
      "outputs": [],
      "source": [
        "# Path\n",
        "MODEL_PATH = \"/content/gdrive/MyDrive/Models/GAN_Scar\"\n",
        "\n",
        "RAW_TRAIN_SET_PATH = \"/Raw_Dataset/train\"\n",
        "RAW_TEST_SET_PATH = \"/Raw_Dataset/test\"\n",
        "\n",
        "DATASET_PATH = \"/Dataset\"\n",
        "\n",
        "IMAGES_PATH = '/images'\n",
        "LABELS_PATH = '/labels'\n",
        "\n",
        "# Image preprocess\n",
        "NORM_INPUT_W_SIZE = 512\n",
        "NORM_INPUT_H_SIZE = 384"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttpnk92dc7Je"
      },
      "source": [
        "# **Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8pYDBzLPc8W2"
      },
      "outputs": [],
      "source": [
        "def imshow_waitkey_enter(image):\n",
        "    cv2_imshow(image)\n",
        "\n",
        "    time.sleep(0.5)\n",
        "    \n",
        "    input(\"Please press the Enter key to proceed\\n\")\n",
        "    output.clear()\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR9mVlktdB-A"
      },
      "source": [
        "# **Make dir**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c3JXzvP6dDFA"
      },
      "outputs": [],
      "source": [
        "# Create processed dataset dir\n",
        "train_dir = os.path.join(MODEL_PATH + DATASET_PATH, 'train')\n",
        "val_dir = os.path.join(MODEL_PATH + DATASET_PATH, 'val')\n",
        "test_dir = os.path.join(MODEL_PATH + DATASET_PATH, 'test')\n",
        "\n",
        "if not os.path.exists(train_dir):\n",
        "    os.makedirs(train_dir)\n",
        "\n",
        "if not os.path.exists(val_dir):\n",
        "    os.makedirs(val_dir)\n",
        "\n",
        "if not os.path.exists(test_dir):\n",
        "    os.makedirs(test_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BULUcWpAkUIK"
      },
      "source": [
        "# 사용 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhiroA0ykUDr"
      },
      "source": [
        "## UNet\n",
        "\n",
        "**마지막 Output 채널 2 → 1 변경**\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=14CzAAaKv5v7pVfvugBRbD1xI4IuhmoyT\"  width = 640>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "yOznWQi4teEI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE4AWru7Aflb"
      },
      "source": [
        "# ***데이터셋 학습 비교 실험***\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "***공통사항***\n",
        "* 원본 이미지 크기 = 244 * 244\n",
        "* epoch = 40\n",
        "* batch = 8\n",
        "> UNet batch size = 4 <br>\n",
        "> Wound dataset batch size = 8\n",
        "* lr = 1e-3\n",
        "* Train:Val = 8:2\n",
        "\n",
        "<br>\n",
        "\n",
        "***주의사항***\n",
        "* 이미지의 사이즈는 정규화 되어야 함\n",
        "\n",
        "<br>\n",
        "\n",
        "***참고사항***\n",
        "* CNN 대표 모델들의 네트워크 입력 사이즈는 224 * 224\n",
        "* 이미지의 사이즈가 큰 경우에, Overlap-Tite(down sampling) 전략으로 한 이미지를 나눠서 학습시킨다.\n",
        "* Polar Res UNet++ 의 input/output 사이즈는 [(384*512) == (y, x)](https://github.com/marinbenc/medical-polar-training/blob/main/datasets/lesion/make_dataset.py#L62)\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "LzjVRDontdRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [대조군](https://www.nature.com/articles/s41598-020-78799-w)\n",
        "> [Train code](https://github.com/uwm-bigdata/wound-segmentation/blob/de3b9c00974b065b2062fd5cd2efe670da1f8f51/train.py)\n",
        "> <br>\n",
        "> [Data generator](https://github.com/uwm-bigdata/wound-segmentation/blob/de3b9c00974b065b2062fd5cd2efe670da1f8f51/utils/io/data.py#L10)\n",
        "> 1. 컬러 이미지(3채널)이 input으로 주어진다.\n",
        "> 2. Raw dataset에 대한 전처리 없이, load 후 model에 들어가기 전에 Normalize\n",
        "\n",
        "```\n",
        "model = Deeplabv3(input_shape=(input_dim_x, input_dim_y, 3), classes=1)\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "***ISIC 2018 1위 모델, Polar Res UNet++ Dice score***\n",
        "> Dice score = 0.925 (Dice loss = 0.075)\n",
        "> <br>\n",
        "> [Input size (y, x) = (384, 512)](https://github.com/marinbenc/medical-polar-training/blob/ecda91650279cd8794080a8ee19c1183faa8d022/datasets/lesion/make_dataset.py#L62)"
      ],
      "metadata": {
        "id": "iuMgswybTBIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Input size =  torch.Size([8, 1, 224, 224])\n",
        "\n",
        "TRAIN || EPOCH 0020 | BATCH  079 /  083 | DICE LOSS  0.1887\n",
        "TRAIN || EPOCH 0020 | BATCH  080 /  083 | DICE LOSS  0.1890\n",
        "TRAIN || EPOCH 0020 | BATCH  081 /  083 | DICE LOSS  0.1885\n",
        "TRAIN || EPOCH 0020 | BATCH  082 /  083 | DICE LOSS  0.1872\n",
        "TRAIN || EPOCH 0020 | BATCH  083 /  083 | DICE LOSS  0.1864\n",
        "\n",
        "VALID || EPOCH 0020 | BATCH  017 /  021 | DICE_LOSS  0.1605\n",
        "VALID || EPOCH 0020 | BATCH  018 /  021 | DICE_LOSS  0.1673\n",
        "VALID || EPOCH 0020 | BATCH  019 /  021 | DICE_LOSS  0.1699\n",
        "VALID || EPOCH 0020 | BATCH  020 /  021 | DICE_LOSS  0.1710\n",
        "VALID || EPOCH 0020 | BATCH  021 /  021 | DICE_LOSS  0.1736\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "d8_SlWxvUPng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arrange dataset\n",
        "> Not change anything about input image(scar)"
      ],
      "metadata": {
        "id": "LAOzIhbp7gxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{val_idx:03d}.png'), image)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{val_idx:03d}.png'), label_image)"
      ],
      "metadata": {
        "id": "UdLe04Tt7MDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = MODEL_PATH + RAW_TEST_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TEST_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "os.chdir(image_path)\n",
        "test_files = os.listdir(image_path)\n",
        "test_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "os.chdir(label_path)\n",
        "test_label_files = os.listdir(label_path)\n",
        "test_label_files.sort()\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(len(test_files)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(test_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(test_label_files[idx])\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(test_dir, f'scar_{idx:03d}.png'), image)\n",
        "    cv2.imwrite(os.path.join(test_dir, f'label_{idx:03d}.png'), label_image)"
      ],
      "metadata": {
        "id": "oFWrZfRe7zpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "tvQyDfhctY8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 실험군"
      ],
      "metadata": {
        "id": "Y4_rZUZrTdSr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZUKRe7MccGb"
      },
      "source": [
        "### 1\n",
        "\n",
        "*적용사항*\n",
        "1. Lab space를 이용한 Contrast 조정\n",
        "2. BGR의 Green channel 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Performance\n",
        "\n",
        "```\n",
        "Input size =  torch.Size([8, 1, 224, 224])\n",
        "\n",
        "TRAIN || EPOCH 0020 | BATCH  079 /  083 | DICE LOSS  0.2303\n",
        "TRAIN || EPOCH 0020 | BATCH  080 /  083 | DICE LOSS  0.2291\n",
        "TRAIN || EPOCH 0020 | BATCH  081 /  083 | DICE LOSS  0.2298\n",
        "TRAIN || EPOCH 0020 | BATCH  082 /  083 | DICE LOSS  0.2293\n",
        "TRAIN || EPOCH 0020 | BATCH  083 /  083 | DICE LOSS  0.2305\n",
        "\n",
        "VALID || EPOCH 0020 | BATCH  017 /  021 | DICE_LOSS  0.2455\n",
        "VALID || EPOCH 0020 | BATCH  018 /  021 | DICE_LOSS  0.2397\n",
        "VALID || EPOCH 0020 | BATCH  019 /  021 | DICE_LOSS  0.2367\n",
        "VALID || EPOCH 0020 | BATCH  020 /  021 | DICE_LOSS  0.2480\n",
        "VALID || EPOCH 0020 | BATCH  021 /  021 | DICE_LOSS  0.2476\n",
        "```"
      ],
      "metadata": {
        "id": "AZ5JiTZ1-Sp3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGnC5SL1dPQA"
      },
      "source": [
        "#### Process dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train"
      ],
      "metadata": {
        "id": "gL9q8KvhsZtD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yljPDLtm0zuo"
      },
      "outputs": [],
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_image = cv2.equalizeHist(l_image)\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    b_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "    b_image = b_clahe.apply(b_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_image = cv2.equalizeHist(l_image)\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    b_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "    b_image = b_clahe.apply(b_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{val_idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{val_idx:03d}.png'), label_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU5G6MgG25A0"
      },
      "source": [
        "##### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIU5Cv4f2wGo"
      },
      "outputs": [],
      "source": [
        "image_path = MODEL_PATH + RAW_TEST_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TEST_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "os.chdir(image_path)\n",
        "test_files = os.listdir(image_path)\n",
        "test_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "os.chdir(label_path)\n",
        "test_label_files = os.listdir(label_path)\n",
        "test_label_files.sort()\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(len(test_files)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(test_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(test_label_files[idx])\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_image = cv2.equalizeHist(l_image)\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    b_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "    b_image = b_clahe.apply(b_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(test_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(test_dir, f'label_{idx:03d}.png'), label_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhgq7zplcjoh"
      },
      "source": [
        "### 2\n",
        "\n",
        "*적용사항*\n",
        "1. 불필요 Black boundary 제거\n",
        "2. Lab space를 이용한 CLAHE Contrast 조정\n",
        "3. BGR의 Green channel 사용\n",
        "4. Resize input/output to 384*512 (y, x)\n",
        "> UNet 아키텍쳐 입력 이미지 사이즈"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Performance\n",
        "\n",
        "```\n",
        "Input size =  torch.Size([8, 1, 384, 512])\n",
        "\n",
        "TRAIN || EPOCH 0020 | BATCH  079 /  083 | DICE LOSS  0.3232\n",
        "TRAIN || EPOCH 0020 | BATCH  080 /  083 | DICE LOSS  0.3245\n",
        "TRAIN || EPOCH 0020 | BATCH  081 /  083 | DICE LOSS  0.3232\n",
        "TRAIN || EPOCH 0020 | BATCH  082 /  083 | DICE LOSS  0.3228\n",
        "TRAIN || EPOCH 0020 | BATCH  083 /  083 | DICE LOSS  0.3211\n",
        "\n",
        "VALID || EPOCH 0020 | BATCH  017 /  021 | DICE_LOSS  0.3212\n",
        "VALID || EPOCH 0020 | BATCH  018 /  021 | DICE_LOSS  0.3204\n",
        "VALID || EPOCH 0020 | BATCH  019 /  021 | DICE_LOSS  0.3244\n",
        "VALID || EPOCH 0020 | BATCH  020 /  021 | DICE_LOSS  0.3239\n",
        "VALID || EPOCH 0020 | BATCH  021 /  021 | DICE_LOSS  0.3235\n",
        "```"
      ],
      "metadata": {
        "id": "bUaRzvyV-V-D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIYSvyIfg37Q"
      },
      "source": [
        "#### Process dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train"
      ],
      "metadata": {
        "id": "MIqYMuSCsftk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecWAX5SGg4Hw"
      },
      "outputs": [],
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    if width > height: height = width\n",
        "    else: width = height\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_image = cv2.equalizeHist(l_image)\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    b_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "    b_image = b_clahe.apply(b_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Resize image\n",
        "    green_image = cv2.resize(green_image, \n",
        "                             (NORM_INPUT_W_SIZE, NORM_INPUT_H_SIZE),\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        "    label_image = cv2.resize(label_image, \n",
        "                             (NORM_INPUT_W_SIZE, NORM_INPUT_H_SIZE),\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    if width > height: height = width\n",
        "    else: width = height\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_image = cv2.equalizeHist(l_image)\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    b_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "    b_image = b_clahe.apply(b_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Resize image\n",
        "    green_image = cv2.resize(green_image, \n",
        "                             (NORM_INPUT_W_SIZE, NORM_INPUT_H_SIZE),\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        "    label_image = cv2.resize(label_image, \n",
        "                             (NORM_INPUT_W_SIZE, NORM_INPUT_H_SIZE),\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{val_idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{val_idx:03d}.png'), label_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9izC65bg7vV"
      },
      "source": [
        "##### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BnUptGVg72g"
      },
      "outputs": [],
      "source": [
        "image_path = MODEL_PATH + RAW_TEST_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TEST_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "os.chdir(image_path)\n",
        "test_files = os.listdir(image_path)\n",
        "test_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "os.chdir(label_path)\n",
        "test_label_files = os.listdir(label_path)\n",
        "test_label_files.sort()\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(len(test_files)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(test_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(test_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    if width > height: height = width\n",
        "    else: width = height\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_image = cv2.equalizeHist(l_image)\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    b_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "    b_image = b_clahe.apply(b_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Resize image\n",
        "    green_image = cv2.resize(green_image, \n",
        "                             (NORM_INPUT_W_SIZE, NORM_INPUT_H_SIZE),\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        "    label_image = cv2.resize(label_image, \n",
        "                             (NORM_INPUT_W_SIZE, NORM_INPUT_H_SIZE),\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(test_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(test_dir, f'label_{idx:03d}.png'), label_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWxTg5BSop6e"
      },
      "source": [
        "### 3 (학습 중)\n",
        "\n",
        "*적용사항*\n",
        "1. Black boundary 모두 제거\n",
        "2. Lab space를 이용한 CLAHE Contrast 조정\n",
        "3. BGR의 Green channel 사용\n",
        "4. Bordering(cv2.copymakeborder CONSTANT) input/output to 384*512 (y, x)\n",
        "> UNet 아키텍쳐 입력 이미지 사이즈"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Performance\n",
        "\n",
        "```\n",
        "Input size =  torch.Size([8, 1, 384, 512])\n",
        "\n",
        "TRAIN || EPOCH 0020 | BATCH  079 /  083 | DICE LOSS  0.2362\n",
        "TRAIN || EPOCH 0020 | BATCH  080 /  083 | DICE LOSS  0.2352\n",
        "TRAIN || EPOCH 0020 | BATCH  081 /  083 | DICE LOSS  0.2347\n",
        "TRAIN || EPOCH 0020 | BATCH  082 /  083 | DICE LOSS  0.2344\n",
        "TRAIN || EPOCH 0020 | BATCH  083 /  083 | DICE LOSS  0.2367\n",
        "\n",
        "VALID || EPOCH 0020 | BATCH  017 /  021 | DICE_LOSS  0.3224\n",
        "VALID || EPOCH 0020 | BATCH  018 /  021 | DICE_LOSS  0.3294\n",
        "VALID || EPOCH 0020 | BATCH  019 /  021 | DICE_LOSS  0.3393\n",
        "VALID || EPOCH 0020 | BATCH  020 /  021 | DICE_LOSS  0.3494\n",
        "VALID || EPOCH 0020 | BATCH  021 /  021 | DICE_LOSS  0.3562\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "1dFrSvaE-aBx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJnrN--ao5xc"
      },
      "source": [
        "#### Process dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train"
      ],
      "metadata": {
        "id": "ox2kye1-smhP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tBf58SJVo5SY"
      },
      "outputs": [],
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_image = cv2.equalizeHist(l_image)\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    b_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "    b_image = b_clahe.apply(b_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_H_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_H_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_W_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_W_SIZE - total_width)\n",
        "\n",
        "    green_image = cv2.copyMakeBorder(green_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_image = cv2.equalizeHist(l_image)\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    b_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "    b_image = b_clahe.apply(b_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_H_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_H_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_W_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_W_SIZE - total_width)\n",
        "\n",
        "    green_image = cv2.copyMakeBorder(green_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{val_idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{val_idx:03d}.png'), label_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl46mYQto6Dr"
      },
      "source": [
        "##### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iGFmaB8Zo6r5"
      },
      "outputs": [],
      "source": [
        "image_path = MODEL_PATH + RAW_TEST_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TEST_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "os.chdir(image_path)\n",
        "test_files = os.listdir(image_path)\n",
        "test_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "os.chdir(label_path)\n",
        "test_label_files = os.listdir(label_path)\n",
        "test_label_files.sort()\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(len(test_files)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(test_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(test_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_image = cv2.equalizeHist(l_image)\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    b_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "    b_image = b_clahe.apply(b_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_H_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_H_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_W_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_W_SIZE - total_width)\n",
        "\n",
        "    green_image = cv2.copyMakeBorder(green_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(test_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(test_dir, f'label_{idx:03d}.png'), label_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4\n",
        "\n",
        "*적용사항*\n",
        "1. Black boundary 모두 제거\n",
        "2. Lab space를 이용한 CLAHE Contrast 조정\n",
        "3. BGR의 Green channel 사용\n",
        "4. Resize input, label image width 또는 heigth 384 (비율 그대로)\n",
        "5. Bordering(cv2.copymakeborder CONSTANT) input/output to 384*512 (y, x)\n",
        "> UNet 아키텍쳐 입력 이미지 사이즈"
      ],
      "metadata": {
        "id": "1Yxo1r0o1gQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Performance\n",
        "\n",
        "```\n",
        "Input size =  torch.Size([8, 1, 572, 572])\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "LmRR68162Q_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Process dataset"
      ],
      "metadata": {
        "id": "BJhV03IK2RTj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train"
      ],
      "metadata": {
        "id": "jBEewoRYss9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    if width > height: height = width\n",
        "    else: width = height\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Resize images\n",
        "    origin_scale = 1.0\n",
        "    iters = 20\n",
        "    for coefficient in range(1, iters + 1):\n",
        "        resize_scale = origin_scale + (coefficient / 10)\n",
        "\n",
        "        resize_w = width * resize_scale\n",
        "        resize_h = height * resize_scale\n",
        "\n",
        "        # Resizing\n",
        "        if coefficient == iters or (resize_w == NORM_INPUT_W_SIZE or resize_h == NORM_INPUT_H_SIZE):\n",
        "            green_image = cv2.resize(green_image, \n",
        "                             (0, 0),\n",
        "                             fx=resize_scale,\n",
        "                             fy=resize_scale,\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        "            label_image = cv2.resize(label_image, \n",
        "                             (0, 0),\n",
        "                             fx=resize_scale,\n",
        "                             fy=resize_scale,\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        "            break\n",
        "        elif resize_w > NORM_INPUT_W_SIZE or resize_h > NORM_INPUT_H_SIZE:\n",
        "            if coefficient - 1 != 0:\n",
        "                resize_scale = resize_scale - ((coefficient - 1) / 10)\n",
        "            else:\n",
        "                resize_scale = origin_scale\n",
        "\n",
        "            green_image = cv2.resize(green_image, \n",
        "                             (0, 0),\n",
        "                             fx=resize_scale,\n",
        "                             fy=resize_scale,\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        "            label_image = cv2.resize(label_image, \n",
        "                             (0, 0),\n",
        "                             fx=resize_scale,\n",
        "                             fy=resize_scale,\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        "            break\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_W_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_W_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_H_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_H_SIZE - total_width)\n",
        "\n",
        "    green_image = cv2.copyMakeBorder(green_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "    \n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    if width > height: height = width\n",
        "    else: width = height\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    \n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Resize\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_W_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_W_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_H_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_H_SIZE - total_width)\n",
        "\n",
        "    green_image = cv2.copyMakeBorder(green_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "    \n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)"
      ],
      "metadata": {
        "id": "FLbx55hB2Rrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Test"
      ],
      "metadata": {
        "id": "tvhoc1by2R82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = MODEL_PATH + RAW_TEST_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TEST_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "os.chdir(image_path)\n",
        "test_files = os.listdir(image_path)\n",
        "test_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "os.chdir(label_path)\n",
        "test_label_files = os.listdir(label_path)\n",
        "test_label_files.sort()\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(len(test_files)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(test_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(test_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "    \n",
        "    # resize\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_W_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_W_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_H_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_H_SIZE - total_width)\n",
        "\n",
        "    green_image = cv2.copyMakeBorder(green_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "    \n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)"
      ],
      "metadata": {
        "id": "PuYwFWWn2SRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5\n",
        "\n",
        "*적용사항*\n",
        "1. Black boundary 모두 제거\n",
        "2. Lab space를 이용한 CLAHE Contrast 조정\n",
        "3. K-Means(k=3)를 이용한 이미지 분류\n",
        "4. Resize input, label image width 또는 heigth 384 (비율 그대로)\n",
        "5. Bordering(cv2.copymakeborder CONSTANT) input/output to 384*512 (y, x)\n",
        "> UNet 아키텍쳐 입력 이미지 사이즈"
      ],
      "metadata": {
        "id": "LqgHSjqj-dBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Performance\n",
        "\n",
        "```\n",
        "Input size =  torch.Size([8, 1, 572, 572])\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "FD1t6uTb_73Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Process dataset"
      ],
      "metadata": {
        "id": "R8DC7EAS_A4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train"
      ],
      "metadata": {
        "id": "K7AtZAoesxyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    \n",
        "    # Clustering use K-means\n",
        "    cluster_data = enhanced_image.reshape((-1, 3)).astype(np.float32)\n",
        "    k = 3\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS|cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0) # 최대 10번 반복하고 1픽셀 이하로 움직이면 종료\n",
        "    ret, c_label, center = cv2.kmeans(cluster_data, \n",
        "                                    k, \n",
        "                                    None, \n",
        "                                    criteria,\n",
        "                                    10,\n",
        "                                    cv2.KMEANS_RANDOM_CENTERS)\n",
        "    \n",
        "    center = np.uint8(center)\n",
        "    kmeans_image = center[c_label.flatten()]\n",
        "    kmeans_image = kmeans_image.reshape((enhanced_image.shape))\n",
        "    kmeans_image = cv2.cvtColor(kmeans_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_W_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_W_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_H_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_H_SIZE - total_width)\n",
        "\n",
        "    kmeans_image = cv2.copyMakeBorder(kmeans_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "    \n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), kmeans_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    \n",
        "    # Clustering use K-means\n",
        "    cluster_data = enhanced_image.reshape((-1, 3)).astype(np.float32)\n",
        "    k = 3\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS|cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0) # 최대 10번 반복하고 1픽셀 이하로 움직이면 종료\n",
        "    ret, c_label, center = cv2.kmeans(cluster_data, \n",
        "                                    k, \n",
        "                                    None, \n",
        "                                    criteria,\n",
        "                                    10,\n",
        "                                    cv2.KMEANS_RANDOM_CENTERS)\n",
        "    \n",
        "    center = np.uint8(center)\n",
        "    kmeans_image = center[c_label.flatten()]\n",
        "    kmeans_image = kmeans_image.reshape((enhanced_image.shape))\n",
        "    kmeans_image = cv2.cvtColor(kmeans_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_W_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_W_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_H_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_H_SIZE - total_width)\n",
        "\n",
        "    kmeans_image = cv2.copyMakeBorder(kmeans_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{idx:03d}.png'), kmeans_image)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{idx:03d}.png'), label_image)"
      ],
      "metadata": {
        "id": "Zgg9Ps9g_Bm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Test"
      ],
      "metadata": {
        "id": "BLmbVKdb_B0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = MODEL_PATH + RAW_TEST_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TEST_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "os.chdir(image_path)\n",
        "test_files = os.listdir(image_path)\n",
        "test_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "os.chdir(label_path)\n",
        "test_label_files = os.listdir(label_path)\n",
        "test_label_files.sort()\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(len(test_files)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(test_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(test_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    \n",
        "    # Clustering use K-means\n",
        "    cluster_data = enhanced_image.reshape((-1, 3)).astype(np.float32)\n",
        "    k = 3\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS|cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0) # 최대 10번 반복하고 1픽셀 이하로 움직이면 종료\n",
        "    ret, c_label, center = cv2.kmeans(cluster_data, \n",
        "                                    k, \n",
        "                                    None, \n",
        "                                    criteria,\n",
        "                                    10,\n",
        "                                    cv2.KMEANS_RANDOM_CENTERS)\n",
        "    \n",
        "    center = np.uint8(center)\n",
        "    kmeans_image = center[c_label.flatten()]\n",
        "    kmeans_image = kmeans_image.reshape((enhanced_image.shape))\n",
        "    kmeans_image = cv2.cvtColor(kmeans_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_W_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_W_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_H_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_H_SIZE - total_width)\n",
        "\n",
        "    kmeans_image = cv2.copyMakeBorder(kmeans_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(test_dir, f'scar_{idx:03d}.png'), kmeans_image)\n",
        "    cv2.imwrite(os.path.join(test_dir, f'label_{idx:03d}.png'), label_image)"
      ],
      "metadata": {
        "id": "OmlRM4vl_CE3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "LAOzIhbp7gxx",
        "MGnC5SL1dPQA",
        "SIYSvyIfg37Q",
        "fJnrN--ao5xc",
        "R8DC7EAS_A4T"
      ],
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMaDLKBmvGUBTIs0C0NVZ5I",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}