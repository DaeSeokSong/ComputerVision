{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaeSeokSong/image-processing/blob/feature%2FUnet-scar/Processor_PerformanceTester_Scar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_SoWfFhxL0-"
      },
      "source": [
        "# **Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpg4AUPixNrD"
      },
      "outputs": [],
      "source": [
        "# Image processing\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import output\n",
        "\n",
        "# ETC\n",
        "import os\n",
        "import time\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OHL2n1YxlH_"
      },
      "source": [
        "## Mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxOqA3DpxlgC",
        "outputId": "be7f5e13-9733-46ea-938b-7be1faeb4a30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqwTBfCyxm3v",
        "outputId": "00566b52-ec3d-48de-e5dc-541d1aa82997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Models/GAN_Scar\n",
            "total 175\n",
            "drwx------ 2 root root  4096 Aug 16 08:53  Dataset\n",
            "-rw------- 1 root root 22891 Sep  1 12:04  Image_segmentation-Scar.ipynb\n",
            "drwx------ 2 root root  4096 Aug 17 05:49  Log\n",
            "-rw------- 1 root root 49019 Sep  2 04:06  Processor_PerformanceTester-Scar.ipynb\n",
            "drwx------ 2 root root  4096 Aug 16 08:39  Raw_Dataset\n",
            "-rw------- 1 root root 16167 Sep  1 12:13  RawDataset_Processor-Scar.ipynb\n",
            "drwx------ 2 root root  4096 Aug 23 14:47  result\n",
            "-rw------- 1 root root 39995 Aug 15 11:40 'UNet architecture.PNG'\n",
            "-rw------- 1 root root 33422 Sep  2 15:31  Unet-Scar.ipynb\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/MyDrive/Models/GAN_Scar\n",
        "!ls -al"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoO0sORTxEIQ"
      },
      "source": [
        "# **Grobal variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9c_qcr4xHQR"
      },
      "outputs": [],
      "source": [
        "# Path\n",
        "MODEL_PATH = \"/content/gdrive/MyDrive/Models/GAN_Scar\"\n",
        "\n",
        "RAW_TRAIN_SET_PATH = \"/Raw_Dataset/train\"\n",
        "RAW_TEST_SET_PATH = \"/Raw_Dataset/test\"\n",
        "\n",
        "DATASET_PATH = \"/Dataset\"\n",
        "\n",
        "IMAGES_PATH = '/images'\n",
        "LABELS_PATH = '/labels'\n",
        "\n",
        "# Image preprocess\n",
        "NORM_INPUT_W_SIZE = 572\n",
        "NORM_INPUT_H_SIZE = 572"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttpnk92dc7Je"
      },
      "source": [
        "# **Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pYDBzLPc8W2"
      },
      "outputs": [],
      "source": [
        "def imshow_waitkey_enter(image):\n",
        "    cv2_imshow(image)\n",
        "\n",
        "    time.sleep(0.5)\n",
        "    \n",
        "    input(\"Please press the Enter key to proceed\\n\")\n",
        "    output.clear()\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR9mVlktdB-A"
      },
      "source": [
        "# **Make dir**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3JXzvP6dDFA"
      },
      "outputs": [],
      "source": [
        "# Create processed dataset dir\n",
        "train_dir = os.path.join(MODEL_PATH + DATASET_PATH, 'train')\n",
        "val_dir = os.path.join(MODEL_PATH + DATASET_PATH, 'val')\n",
        "test_dir = os.path.join(MODEL_PATH + DATASET_PATH, 'test')\n",
        "\n",
        "if not os.path.exists(train_dir):\n",
        "    os.makedirs(train_dir)\n",
        "\n",
        "if not os.path.exists(val_dir):\n",
        "    os.makedirs(val_dir)\n",
        "\n",
        "if not os.path.exists(test_dir):\n",
        "    os.makedirs(test_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BULUcWpAkUIK"
      },
      "source": [
        "# 사용 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhiroA0ykUDr"
      },
      "source": [
        "## UNet\n",
        "\n",
        "**마지막 Output 채널 2 → 1 변경**\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=14CzAAaKv5v7pVfvugBRbD1xI4IuhmoyT\"  width = 640>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "yOznWQi4teEI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE4AWru7Aflb"
      },
      "source": [
        "# ***데이터셋 학습 비교 실험***\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "***공통사항***\n",
        "* 원본 이미지 크기 = 244 * 244\n",
        "* epoch = 40\n",
        "* batch = 8\n",
        "> UNet batch size = 4 <br>\n",
        "> Wound dataset batch size = 8\n",
        "* lr = 1e-3\n",
        "\n",
        "<br>\n",
        "\n",
        "***주의사항***\n",
        "* 이미지의 사이즈는 정규화 되어야 함\n",
        "\n",
        "<br>\n",
        "\n",
        "***참고사항***\n",
        "* CNN 대표 모델들의 네트워크 입력 사이즈는 224 * 224\n",
        "* 이미지의 사이즈가 큰 경우에, Overlap-Tite(down sampling) 전략으로 한 이미지를 나눠서 학습시킨다.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "LzjVRDontdRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [대조군](https://www.nature.com/articles/s41598-020-78799-w)"
      ],
      "metadata": {
        "id": "iuMgswybTBIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance"
      ],
      "metadata": {
        "id": "d8_SlWxvUPng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process dataset"
      ],
      "metadata": {
        "id": "MwiUrNlkURTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train"
      ],
      "metadata": {
        "id": "F7ece13UtUMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    \n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    \n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{val_idx:03d}.png'), image)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{val_idx:03d}.png'), label_image)"
      ],
      "metadata": {
        "id": "nZziuYTsTBWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test"
      ],
      "metadata": {
        "id": "9uR09FRUUVd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = MODEL_PATH + RAW_TEST_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TEST_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "os.chdir(image_path)\n",
        "test_files = os.listdir(image_path)\n",
        "test_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "os.chdir(label_path)\n",
        "test_label_files = os.listdir(label_path)\n",
        "test_label_files.sort()\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(len(test_files)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(test_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(test_label_files[idx])\n",
        "\n",
        "    \n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(test_dir, f'scar_{idx:03d}.png'), image)\n",
        "    cv2.imwrite(os.path.join(test_dir, f'label_{idx:03d}.png'), label_image)"
      ],
      "metadata": {
        "id": "ZHOf7nSrUVw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "tvQyDfhctY8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 실험군"
      ],
      "metadata": {
        "id": "Y4_rZUZrTdSr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZUKRe7MccGb"
      },
      "source": [
        "### 1\n",
        "\n",
        "*적용사항*\n",
        "1. Lab space를 이용한 Contrast 조정\n",
        "2. BGR의 Green channel 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Performance\n",
        "\n",
        "```\n",
        "Input size =  torch.Size([8, 1, 224, 224])\n",
        "\n",
        "TRAIN || EPOCH 0020 | BATCH  069 /  073 | LOSS  0.6762\n",
        "TRAIN || EPOCH 0020 | BATCH  070 /  073 | LOSS  0.6762\n",
        "TRAIN || EPOCH 0020 | BATCH  071 /  073 | LOSS  0.6762\n",
        "TRAIN || EPOCH 0020 | BATCH  072 /  073 | LOSS  0.6762\n",
        "TRAIN || EPOCH 0020 | BATCH  073 /  073 | LOSS  0.6762\n",
        "\n",
        "VALID || EPOCH 0020 | BATCH  028 /  032 | LOSS  0.6805\n",
        "VALID || EPOCH 0020 | BATCH  029 /  032 | LOSS  0.6805\n",
        "VALID || EPOCH 0020 | BATCH  030 /  032 | LOSS  0.6806\n",
        "VALID || EPOCH 0020 | BATCH  031 /  032 | LOSS  0.6807\n",
        "VALID || EPOCH 0020 | BATCH  032 /  032 | LOSS  0.6805\n",
        "```\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1GOlvg4VIjbExmrc3OK0qB90d1_fFt0Q7\"  width = 400>\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1AvmMcgoEUfnPNYgMO4WW6iYiynyJKHE3\"  width = 800>"
      ],
      "metadata": {
        "id": "AZ5JiTZ1-Sp3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGnC5SL1dPQA"
      },
      "source": [
        "#### Process dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train"
      ],
      "metadata": {
        "id": "gL9q8KvhsZtD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yljPDLtm0zuo"
      },
      "outputs": [],
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_image = cv2.equalizeHist(l_image)\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    b_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "    b_image = b_clahe.apply(b_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_image = cv2.equalizeHist(l_image)\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    b_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "    b_image = b_clahe.apply(b_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{val_idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{val_idx:03d}.png'), label_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU5G6MgG25A0"
      },
      "source": [
        "##### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIU5Cv4f2wGo"
      },
      "outputs": [],
      "source": [
        "image_path = MODEL_PATH + RAW_TEST_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TEST_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "os.chdir(image_path)\n",
        "test_files = os.listdir(image_path)\n",
        "test_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "os.chdir(label_path)\n",
        "test_label_files = os.listdir(label_path)\n",
        "test_label_files.sort()\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(len(test_files)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(test_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(test_label_files[idx])\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_image = cv2.equalizeHist(l_image)\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    b_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "    b_image = b_clahe.apply(b_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(test_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(test_dir, f'label_{idx:03d}.png'), label_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhgq7zplcjoh"
      },
      "source": [
        "### 2\n",
        "\n",
        "*적용사항*\n",
        "1. 불필요 Black boundary 제거\n",
        "2. Lab space를 이용한 CLAHE Contrast 조정\n",
        "3. BGR의 Green channel 사용\n",
        "4. Resize input to 572*572\n",
        "5. Resize output to 388*388\n",
        "> UNet 아키텍쳐 입력 이미지 사이즈"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Performance\n",
        "\n",
        "```\n",
        "Input size =  torch.Size([8, 1, 572, 572])\n",
        "\n",
        "TRAIN || EPOCH 0020 | BATCH  069 /  073 | LOSS  0.5164\n",
        "TRAIN || EPOCH 0020 | BATCH  070 /  073 | LOSS  0.5159\n",
        "TRAIN || EPOCH 0020 | BATCH  071 /  073 | LOSS  0.5155\n",
        "TRAIN || EPOCH 0020 | BATCH  072 /  073 | LOSS  0.5150\n",
        "TRAIN || EPOCH 0020 | BATCH  073 /  073 | LOSS  0.5146\n",
        "\n",
        "VALID || EPOCH 0020 | BATCH  028 /  032 | LOSS  0.5083\n",
        "VALID || EPOCH 0020 | BATCH  029 /  032 | LOSS  0.5070\n",
        "VALID || EPOCH 0020 | BATCH  030 /  032 | LOSS  0.5049\n",
        "VALID || EPOCH 0020 | BATCH  031 /  032 | LOSS  0.5052\n",
        "VALID || EPOCH 0020 | BATCH  032 /  032 | LOSS  0.5043\n",
        "```\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1uiIqGtvEmYWUhBbjKwjDtncDfg66b1cw\"  width = 400>\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1Q5tzGUKwYX5rX4Viy12zHBPukUx1AjtO\"  width = 800>"
      ],
      "metadata": {
        "id": "bUaRzvyV-V-D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIYSvyIfg37Q"
      },
      "source": [
        "#### Process dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train"
      ],
      "metadata": {
        "id": "MIqYMuSCsftk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecWAX5SGg4Hw"
      },
      "outputs": [],
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    if width > height: height = width\n",
        "    else: width = height\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_image = cv2.equalizeHist(l_image)\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    b_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "    b_image = b_clahe.apply(b_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Resize image\n",
        "    green_image = resize_image(green_image)\n",
        "    label_image = resize_image(label_image)\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    if width > height: height = width\n",
        "    else: width = height\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_image = cv2.equalizeHist(l_image)\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    b_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "    b_image = b_clahe.apply(b_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Resize image\n",
        "    green_image = resize_image(green_image)\n",
        "    label_image = resize_image(label_image)\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{val_idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{val_idx:03d}.png'), label_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9izC65bg7vV"
      },
      "source": [
        "##### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BnUptGVg72g"
      },
      "outputs": [],
      "source": [
        "image_path = MODEL_PATH + RAW_TEST_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TEST_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "os.chdir(image_path)\n",
        "test_files = os.listdir(image_path)\n",
        "test_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "os.chdir(label_path)\n",
        "test_label_files = os.listdir(label_path)\n",
        "test_label_files.sort()\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(len(test_files)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(test_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(test_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    if width > height: height = width\n",
        "    else: width = height\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_image = cv2.equalizeHist(l_image)\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    b_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "    b_image = b_clahe.apply(b_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Resize image\n",
        "    green_image = resize_image(green_image)\n",
        "    label_image = resize_image(label_image)\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(test_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(test_dir, f'label_{idx:03d}.png'), label_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWxTg5BSop6e"
      },
      "source": [
        "### 3\n",
        "\n",
        "*적용사항*\n",
        "1. Black boundary 모두 제거\n",
        "2. Lab space를 이용한 CLAHE Contrast 조정\n",
        "3. BGR의 Green channel 사용\n",
        "4. Bordering(cv2.copymakeborder CONSTANT) input to 572*572\n",
        "5. Resize output to 388*388\n",
        "> UNet 아키텍쳐 입력 이미지 사이즈"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Performance\n",
        "\n",
        "```\n",
        "Input size =  torch.Size([8, 1, 572, 572])\n",
        "\n",
        "TRAIN || EPOCH 0020 | BATCH  069 /  073 | LOSS  0.3800\n",
        "TRAIN || EPOCH 0020 | BATCH  070 /  073 | LOSS  0.3800\n",
        "TRAIN || EPOCH 0020 | BATCH  071 /  073 | LOSS  0.3800\n",
        "TRAIN || EPOCH 0020 | BATCH  072 /  073 | LOSS  0.3800\n",
        "TRAIN || EPOCH 0020 | BATCH  073 /  073 | LOSS  0.3800\n",
        "\n",
        "VALID || EPOCH 0020 | BATCH  028 /  032 | LOSS  0.3811\n",
        "VALID || EPOCH 0020 | BATCH  029 /  032 | LOSS  0.3811\n",
        "VALID || EPOCH 0020 | BATCH  030 /  032 | LOSS  0.3811\n",
        "VALID || EPOCH 0020 | BATCH  031 /  032 | LOSS  0.3810\n",
        "VALID || EPOCH 0020 | BATCH  032 /  032 | LOSS  0.3810\n",
        "```\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=170ISySZSU8JKrOss--9rPujTHRf7TI0i\"  width = 400>\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1g3Wbe6ym3VK0srroYD-bnwSbMy9QYLW5\"  width = 800>"
      ],
      "metadata": {
        "id": "1dFrSvaE-aBx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJnrN--ao5xc"
      },
      "source": [
        "#### Process dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train"
      ],
      "metadata": {
        "id": "ox2kye1-smhP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBf58SJVo5SY"
      },
      "outputs": [],
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_W_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_W_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_H_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_H_SIZE - total_width)\n",
        "\n",
        "    image = cv2.copyMakeBorder(image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_image = cv2.equalizeHist(l_image)\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    b_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "    b_image = b_clahe.apply(b_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_W_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_W_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_H_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_H_SIZE - total_width)\n",
        "\n",
        "    image = cv2.copyMakeBorder(image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_image = cv2.equalizeHist(l_image)\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    b_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "    b_image = b_clahe.apply(b_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{val_idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{val_idx:03d}.png'), label_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl46mYQto6Dr"
      },
      "source": [
        "##### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGFmaB8Zo6r5"
      },
      "outputs": [],
      "source": [
        "image_path = MODEL_PATH + RAW_TEST_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TEST_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "os.chdir(image_path)\n",
        "test_files = os.listdir(image_path)\n",
        "test_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "os.chdir(label_path)\n",
        "test_label_files = os.listdir(label_path)\n",
        "test_label_files.sort()\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(len(test_files)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(test_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(test_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_W_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_W_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_H_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_H_SIZE - total_width)\n",
        "\n",
        "    image = cv2.copyMakeBorder(image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_image = cv2.equalizeHist(l_image)\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    b_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "    b_image = b_clahe.apply(b_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(test_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(test_dir, f'label_{idx:03d}.png'), label_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4\n",
        "\n",
        "*적용사항*\n",
        "1. Black boundary 모두 제거\n",
        "2. Lab space를 이용한 CLAHE Contrast 조정\n",
        "3. BGR의 Green channel 사용\n",
        "4. Resize input, label image width 또는 heigth 512 (비율 그대로)\n",
        "5. Bordering(cv2.copymakeborder CONSTANT) input to 572*572\n",
        "6. Resize output to 388*388\n",
        "> UNet 아키텍쳐 입력 이미지 사이즈"
      ],
      "metadata": {
        "id": "1Yxo1r0o1gQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Performance"
      ],
      "metadata": {
        "id": "LmRR68162Q_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Process dataset"
      ],
      "metadata": {
        "id": "BJhV03IK2RTj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train"
      ],
      "metadata": {
        "id": "jBEewoRYss9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    if width > height: height = width\n",
        "    else: width = height\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    \n",
        "    # Resize images\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_W_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_W_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_H_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_H_SIZE - total_width)\n",
        "\n",
        "    kmeans_image = cv2.copyMakeBorder(kmeans_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "    \n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), kmeans_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    if width > height: height = width\n",
        "    else: width = height\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    \n",
        "    # resize\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_W_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_W_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_H_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_H_SIZE - total_width)\n",
        "\n",
        "    kmeans_image = cv2.copyMakeBorder(kmeans_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "    \n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), kmeans_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)"
      ],
      "metadata": {
        "id": "FLbx55hB2Rrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Test"
      ],
      "metadata": {
        "id": "tvhoc1by2R82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = MODEL_PATH + RAW_TEST_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TEST_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "os.chdir(image_path)\n",
        "test_files = os.listdir(image_path)\n",
        "test_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "os.chdir(label_path)\n",
        "test_label_files = os.listdir(label_path)\n",
        "test_label_files.sort()\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(len(test_files)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(test_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(test_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    \n",
        "    # resize\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_W_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_W_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_H_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_H_SIZE - total_width)\n",
        "\n",
        "    kmeans_image = cv2.copyMakeBorder(kmeans_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "    \n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), kmeans_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)"
      ],
      "metadata": {
        "id": "PuYwFWWn2SRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5\n",
        "\n",
        "*적용사항*\n",
        "1. Black boundary 모두 제거\n",
        "2. Lab space를 이용한 CLAHE Contrast 조정\n",
        "3. K-Means(k=3)를 이용한 이미지 분류\n",
        "4. Resize input, label image width 또는 heigth 512 (비율 그대로)\n",
        "5. Bordering(cv2.copymakeborder CONSTANT) input to 572*572\n",
        "6. Resize output to 388*388\n",
        "> UNet 아키텍쳐 입력 이미지 사이즈"
      ],
      "metadata": {
        "id": "LqgHSjqj-dBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Performance"
      ],
      "metadata": {
        "id": "FD1t6uTb_73Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Process dataset"
      ],
      "metadata": {
        "id": "R8DC7EAS_A4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train"
      ],
      "metadata": {
        "id": "K7AtZAoesxyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    \n",
        "    # Clustering use K-means\n",
        "    cluster_data = enhanced_image.reshape((-1, 3)).astype(np.float32)\n",
        "    k = 3\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS|cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0) # 최대 10번 반복하고 1픽셀 이하로 움직이면 종료\n",
        "    ret, c_label, center = cv2.kmeans(cluster_data, \n",
        "                                    k, \n",
        "                                    None, \n",
        "                                    criteria,\n",
        "                                    10,\n",
        "                                    cv2.KMEANS_RANDOM_CENTERS)\n",
        "    \n",
        "    center = np.uint8(center)\n",
        "    kmeans_image = center[c_label.flatten()]\n",
        "    kmeans_image = kmeans_image.reshape((enhanced_image.shape))\n",
        "    kmeans_image = cv2.cvtColor(kmeans_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_W_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_W_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_H_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_H_SIZE - total_width)\n",
        "\n",
        "    kmeans_image = cv2.copyMakeBorder(kmeans_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "    \n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), kmeans_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    \n",
        "    # Clustering use K-means\n",
        "    cluster_data = enhanced_image.reshape((-1, 3)).astype(np.float32)\n",
        "    k = 3\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS|cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0) # 최대 10번 반복하고 1픽셀 이하로 움직이면 종료\n",
        "    ret, c_label, center = cv2.kmeans(cluster_data, \n",
        "                                    k, \n",
        "                                    None, \n",
        "                                    criteria,\n",
        "                                    10,\n",
        "                                    cv2.KMEANS_RANDOM_CENTERS)\n",
        "    \n",
        "    center = np.uint8(center)\n",
        "    kmeans_image = center[c_label.flatten()]\n",
        "    kmeans_image = kmeans_image.reshape((enhanced_image.shape))\n",
        "    kmeans_image = cv2.cvtColor(kmeans_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_W_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_W_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_H_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_H_SIZE - total_width)\n",
        "\n",
        "    kmeans_image = cv2.copyMakeBorder(kmeans_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{idx:03d}.png'), kmeans_image)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{idx:03d}.png'), label_image)"
      ],
      "metadata": {
        "id": "Zgg9Ps9g_Bm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Test"
      ],
      "metadata": {
        "id": "BLmbVKdb_B0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = MODEL_PATH + RAW_TEST_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TEST_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "os.chdir(image_path)\n",
        "test_files = os.listdir(image_path)\n",
        "test_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "os.chdir(label_path)\n",
        "test_label_files = os.listdir(label_path)\n",
        "test_label_files.sort()\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(len(test_files)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(test_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(test_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    \n",
        "    # Clustering use K-means\n",
        "    cluster_data = enhanced_image.reshape((-1, 3)).astype(np.float32)\n",
        "    k = 3\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS|cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0) # 최대 10번 반복하고 1픽셀 이하로 움직이면 종료\n",
        "    ret, c_label, center = cv2.kmeans(cluster_data, \n",
        "                                    k, \n",
        "                                    None, \n",
        "                                    criteria,\n",
        "                                    10,\n",
        "                                    cv2.KMEANS_RANDOM_CENTERS)\n",
        "    \n",
        "    center = np.uint8(center)\n",
        "    kmeans_image = center[c_label.flatten()]\n",
        "    kmeans_image = kmeans_image.reshape((enhanced_image.shape))\n",
        "    kmeans_image = cv2.cvtColor(kmeans_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_W_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_W_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_H_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_H_SIZE - total_width)\n",
        "\n",
        "    kmeans_image = cv2.copyMakeBorder(kmeans_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(test_dir, f'scar_{idx:03d}.png'), kmeans_image)\n",
        "    cv2.imwrite(os.path.join(test_dir, f'label_{idx:03d}.png'), label_image)"
      ],
      "metadata": {
        "id": "OmlRM4vl_CE3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Y4_rZUZrTdSr"
      ],
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNQxavR/SJ4yPh71YNeY4Uc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}