{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaeSeokSong/image-processing/blob/feature%2FPreprocessing/Preprocessor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install**"
      ],
      "metadata": {
        "id": "rmvVB2P2aWlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LZNy7tJjaUz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_SoWfFhxL0-"
      },
      "source": [
        "# **Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpg4AUPixNrD"
      },
      "outputs": [],
      "source": [
        "# Image processing\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import output\n",
        "\n",
        "# ETC\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OHL2n1YxlH_"
      },
      "source": [
        "## Mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxOqA3DpxlgC",
        "outputId": "ca7c0d97-2a84-4744-85fd-b7cec03e1cc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqwTBfCyxm3v",
        "outputId": "655539aa-8d2b-48b1-baf8-cb8cb2210451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Models/GAN_Scar\n",
            "total 10890\n",
            "drwx------ 2 root root     4096 Sep 14 06:57  Dataset\n",
            "-rw------- 1 root root    22891 Sep  1 12:04  Image_segmentation-Scar.ipynb\n",
            "drwx------ 2 root root     4096 Aug 17 05:49  Log\n",
            "-rw------- 1 root root    99753 Sep 14 08:48  Processor_PerformanceTester-Scar.ipynb\n",
            "drwx------ 4 root root     4096 Sep 14 06:04 'Raw Dataset'\n",
            "-rw------- 1 root root    16167 Sep  1 12:13  RawDataset_Processor-Scar.ipynb\n",
            "-rw------- 1 root root    39995 Aug 15 11:40 'UNet architecture.PNG'\n",
            "-rw------- 1 root root 10959279 Sep 14 07:49  Unet-Scar.ipynb\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/MyDrive/Models/GAN_Scar\n",
        "!ls -al"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoO0sORTxEIQ"
      },
      "source": [
        "# **Grobal variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9c_qcr4xHQR"
      },
      "outputs": [],
      "source": [
        "# Path\n",
        "MODEL_PATH = \"/content/gdrive/MyDrive/Models/GAN_Scar\"\n",
        "\n",
        "RAW_DATA_PATH = \"/Raw Dataset\"\n",
        "\n",
        "WOUND_TRAIN_PATH = \"/Wound/train\"\n",
        "WOUND_TEST_PATH = \"/Wound/test\"\n",
        "CVC_INPUT_PATH = \"/CVC-clinicDB/Original\"\n",
        "CVC_GT_PATH = \"/CVC-clinicDB/Ground Truth\"\n",
        "\n",
        "DATASET_PATH = \"/Dataset\"\n",
        "\n",
        "IMAGES_PATH = '/images'\n",
        "LABELS_PATH = '/labels'\n",
        "\n",
        "# Image preprocess\n",
        "NORM_INPUT_W_SIZE = 224\n",
        "NORM_INPUT_H_SIZE = 224"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttpnk92dc7Je"
      },
      "source": [
        "# **Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pYDBzLPc8W2"
      },
      "outputs": [],
      "source": [
        "def imshow_waitkey_enter(image):\n",
        "    cv2_imshow(image)\n",
        "\n",
        "    time.sleep(0.5)\n",
        "    \n",
        "    input(\"Please press the Enter key to proceed\\n\")\n",
        "    output.clear()\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR9mVlktdB-A"
      },
      "source": [
        "# **Make dir**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3JXzvP6dDFA"
      },
      "outputs": [],
      "source": [
        "# Create processed dataset dir\n",
        "train_dir = os.path.join(MODEL_PATH + DATASET_PATH, 'train')\n",
        "val_dir = os.path.join(MODEL_PATH + DATASET_PATH, 'val')\n",
        "test_dir = os.path.join(MODEL_PATH + DATASET_PATH, 'test')\n",
        "\n",
        "if not os.path.exists(train_dir):\n",
        "    os.makedirs(train_dir)\n",
        "\n",
        "if not os.path.exists(val_dir):\n",
        "    os.makedirs(val_dir)\n",
        "\n",
        "if not os.path.exists(test_dir):\n",
        "    os.makedirs(test_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE4AWru7Aflb"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "# ***Preprocess Performace Compare Test***\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "***공통사항***\n",
        "* 원본 이미지 크기 = 244 * 244\n",
        "* epoch = 40\n",
        "* batch = 8\n",
        "> UNet batch size = 4 <br>\n",
        "> Wound dataset batch size = 8\n",
        "* lr = 1e-3\n",
        "* Train:Val = 8:2\n",
        "* Used model = Polar Res UNet++ (ISIC 2018 dice scroe rank no.1)\n",
        "\n",
        "<br>\n",
        "\n",
        "***주의사항***\n",
        "* 이미지의 사이즈는 정규화 되어야 한다.\n",
        "* Convolution에서 특징점을 잘 추출하기 때문에 색상영역이나, 스무딩/샤프닝 같은 필터 처리는 되려 성능저하 요소가 될 수 있다. \n",
        "\n",
        "<br>\n",
        "\n",
        "***참고사항***\n",
        "* CNN 대표 모델들의 네트워크 입력 사이즈는 224 * 224\n",
        "* 이미지의 사이즈가 큰 경우에, Overlap-Tite(down sampling) 전략으로 한 이미지를 나눠서 학습시킨다.\n",
        "* Polar Res UNet++ 의 input/output 사이즈는 [(384*512) == (y, x)](https://github.com/marinbenc/medical-polar-training/blob/main/datasets/lesion/make_dataset.py#L62)\n",
        "> Dice score = 0.925 (Dice loss = 0.075)\n",
        "\n",
        "<br>\n",
        "\n",
        "***현재 결과***\n",
        "* Resize 하면 loss가 높아져서 안 좋다.\n",
        "> Resize가 아닌 Bordering을 통해 Size를 조정해야한다.\n",
        "* Color나 Grayscale이나 성능차이가 안 난다.\n",
        "> Grayscale이 학습이 빠르므로 Grayscale로 학습한다.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "## [***Control Group***](https://www.nature.com/articles/s41598-020-78799-w)\n",
        "> [Train code](https://github.com/uwm-bigdata/wound-segmentation/blob/de3b9c00974b065b2062fd5cd2efe670da1f8f51/train.py)\n",
        "> <br>\n",
        "> [Data generator](https://github.com/uwm-bigdata/wound-segmentation/blob/de3b9c00974b065b2062fd5cd2efe670da1f8f51/utils/io/data.py#L10)\n",
        "> <br>\n",
        "> <br>\n",
        "> Raw dataset에 대한 전처리 없이, load 후 model에 들어가기 전에 Normalize\n",
        "> <br>\n",
        "> * 현재까지 대조군 성능을 넘은 실험군이 없다.\n",
        "\n",
        "```\n",
        "model = Deeplabv3(input_shape=(input_dim_x, input_dim_y, 3), classes=1)\n",
        "```"
      ],
      "metadata": {
        "id": "iuMgswybTBIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance\n",
        "\n",
        "<br>\n",
        "\n",
        "**Gray scale**\n",
        "```\n",
        "Input size =  torch.Size([8, 1, 224, 224])\n",
        "\n",
        "TRAIN || EPOCH 0020 | BATCH  079 /  083 | DICE LOSS  0.1887\n",
        "TRAIN || EPOCH 0020 | BATCH  080 /  083 | DICE LOSS  0.1890\n",
        "TRAIN || EPOCH 0020 | BATCH  081 /  083 | DICE LOSS  0.1885\n",
        "TRAIN || EPOCH 0020 | BATCH  082 /  083 | DICE LOSS  0.1872\n",
        "TRAIN || EPOCH 0020 | BATCH  083 /  083 | DICE LOSS  0.1864\n",
        "\n",
        "VALID || EPOCH 0020 | BATCH  017 /  021 | DICE_LOSS  0.1605\n",
        "VALID || EPOCH 0020 | BATCH  018 /  021 | DICE_LOSS  0.1673\n",
        "VALID || EPOCH 0020 | BATCH  019 /  021 | DICE_LOSS  0.1699\n",
        "VALID || EPOCH 0020 | BATCH  020 /  021 | DICE_LOSS  0.1710\n",
        "VALID || EPOCH 0020 | BATCH  021 /  021 | DICE_LOSS  0.1736\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "**Color**\n",
        "```\n",
        "Input size =  torch.Size([8, 3, 224, 224])\n",
        "\n",
        "TRAIN || EPOCH 0020 | BATCH  079 /  083 | DICE LOSS  0.1855\n",
        "TRAIN || EPOCH 0020 | BATCH  080 /  083 | DICE LOSS  0.1848\n",
        "TRAIN || EPOCH 0020 | BATCH  081 /  083 | DICE LOSS  0.1838\n",
        "TRAIN || EPOCH 0020 | BATCH  082 /  083 | DICE LOSS  0.1835\n",
        "TRAIN || EPOCH 0020 | BATCH  083 /  083 | DICE LOSS  0.1824\n",
        "\n",
        "VALID || EPOCH 0020 | BATCH  017 /  021 | DICE_LOSS  0.1781\n",
        "VALID || EPOCH 0020 | BATCH  018 /  021 | DICE_LOSS  0.1831\n",
        "VALID || EPOCH 0020 | BATCH  019 /  021 | DICE_LOSS  0.1798\n",
        "VALID || EPOCH 0020 | BATCH  020 /  021 | DICE_LOSS  0.1789\n",
        "VALID || EPOCH 0020 | BATCH  021 /  021 | DICE_LOSS  0.1865\n",
        "\n",
        "Best loss in Train =  0.1824\n",
        "Best loss in Validation =  0.1812\n",
        "```"
      ],
      "metadata": {
        "id": "d8_SlWxvUPng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arrange dataset\n",
        "> Not change anything about input image(scar)"
      ],
      "metadata": {
        "id": "LAOzIhbp7gxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set load image dir path\n",
        "train_image_path = MODEL_PATH + RAW_DATA_PATH + WOUND_TRAIN_PATH + IMAGES_PATH\n",
        "train_label_path = MODEL_PATH + RAW_DATA_PATH + WOUND_TRAIN_PATH + LABELS_PATH\n",
        "test_image_path = MODEL_PATH + RAW_DATA_PATH + WOUND_TEST_PATH + IMAGES_PATH\n",
        "test_label_path = MODEL_PATH + RAW_DATA_PATH + WOUND_TEST_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw input images for Train\n",
        "inputs_train = os.listdir(train_image_path)\n",
        "inputs_train.sort()\n",
        "\n",
        "# Load raw label images for Train\n",
        "labels_train = os.listdir(train_label_path)\n",
        "labels_train.sort()\n",
        "\n",
        "# Load raw input images for Test\n",
        "input_test = os.listdir(test_image_path)\n",
        "input_test.sort()\n",
        "\n",
        "# Load raw label images for Test\n",
        "label_test = os.listdir(test_label_path)\n",
        "label_test.sort()\n",
        "\n",
        "# Split train:val:test = 8:2\n",
        "input_train, input_val, label_train, label_val = train_test_split(inputs_train, labels_train, test_size=0.2, random_state=1)\n",
        "\n",
        "# Merge train/val/test set\n",
        "train_dataset = dict(zip(input_train, label_train))\n",
        "val_dataset = dict(zip(input_val, label_val))\n",
        "test_dataset = dict(zip(input_test, label_test))\n",
        "\n",
        "# ================================ Train ================================\n",
        "# Init image index\n",
        "idx = 0\n",
        "\n",
        "# Create preprocessed scar image\n",
        "for input, label in train_dataset.items(): \n",
        "    # Access input image\n",
        "    os.chdir(train_image_path)\n",
        "    image = cv2.imread(input)\n",
        "\n",
        "    # Access label image\n",
        "    os.chdir(train_label_path)\n",
        "    label_image = cv2.imread(label)\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "    idx += 1\n",
        "\n",
        "# ============================== Validation ==============================\n",
        "# Init image index\n",
        "idx = 0\n",
        "\n",
        "# Create preprocessed scar image\n",
        "for input, label in val_dataset.items(): \n",
        "    # Access input image\n",
        "    os.chdir(train_image_path)\n",
        "    image = cv2.imread(input)\n",
        "\n",
        "    # Access label image\n",
        "    os.chdir(train_label_path)\n",
        "    label_image = cv2.imread(label)\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{idx:03d}.png'), image)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{idx:03d}.png'), label_image)\n",
        "    idx += 1\n",
        "\n",
        "# ================================= Test =================================\n",
        "# Init image index\n",
        "idx = 0\n",
        "\n",
        "# Create preprocessed scar image\n",
        "for input, label in test_dataset.items(): \n",
        "    # Access input image\n",
        "    os.chdir(test_image_path)\n",
        "    image = cv2.imread(input)\n",
        "\n",
        "    # Access label image\n",
        "    os.chdir(test_label_path)\n",
        "    label_image = cv2.imread(label)\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(test_dir, f'scar_{idx:03d}.png'), image)\n",
        "    cv2.imwrite(os.path.join(test_dir, f'label_{idx:03d}.png'), label_image)\n",
        "    idx += 1"
      ],
      "metadata": {
        "id": "UdLe04Tt7MDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Experimental Group***"
      ],
      "metadata": {
        "id": "sHPbY0FsOzj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case 1\n",
        "\n",
        "1. Delete Black Boundary(Padding)\n",
        "2. Resize Scar Image to (224, 224)"
      ],
      "metadata": {
        "id": "QcitBtU5PAHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case 2\n",
        "\n",
        "1. Delete Black Boundary(Padding)\n",
        "2. Pad the rest part to (224, 224) based on the middle of the image"
      ],
      "metadata": {
        "id": "ZzgNKc_5QSFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case 3\n",
        "\n",
        "1. Delete Black Boundary(Padding)\n",
        "2. Resize Scar Image to (224, 224) in proportion to the maximum ratio\n",
        "3. Pad the rest part to (224, 224) in the direction of right/down "
      ],
      "metadata": {
        "id": "SJ_zsDTRPTNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case 4\n",
        "\n",
        "1. Delete Black Boundary(Padding)\n",
        "2. Resize Scar Image to (224, 224) in proportion to the maximum ratio\n",
        "3. Pad the rest part to (224, 224) based on the middle of the image"
      ],
      "metadata": {
        "id": "Lm8__jJHPrdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case 5\n",
        "\n",
        "1. Delete Black Boundary(Padding)\n",
        "2. Resize Scar Image to 224 based on the smaller size(row or column), keeping the proportion\n",
        "3. Sampling by moving the kernel both on input and label, Then merge it to fit the label and input pair"
      ],
      "metadata": {
        "id": "apS4MTgAQaez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "# **Legacy System**"
      ],
      "metadata": {
        "id": "W7ranE7jK6DM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 실험군 (Grayscale)"
      ],
      "metadata": {
        "id": "Y4_rZUZrTdSr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZUKRe7MccGb"
      },
      "source": [
        "### Case 1\n",
        "\n",
        "*적용사항*\n",
        "1. Lab space의 l, a 채널 CLAHE Contrast 조정\n",
        "2. BGR의 Green channel을 CLAHE Contrast 조정 후 input으로 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Performance\n",
        "\n",
        "```\n",
        "Input size =  torch.Size([8, 1, 224, 224])\n",
        "\n",
        "TRAIN || EPOCH 0020 | BATCH  079 /  083 | DICE LOSS  0.2240\n",
        "TRAIN || EPOCH 0020 | BATCH  080 /  083 | DICE LOSS  0.2236\n",
        "TRAIN || EPOCH 0020 | BATCH  081 /  083 | DICE LOSS  0.2238\n",
        "TRAIN || EPOCH 0020 | BATCH  082 /  083 | DICE LOSS  0.2234\n",
        "TRAIN || EPOCH 0020 | BATCH  083 /  083 | DICE LOSS  0.2225\n",
        "\n",
        "VALID || EPOCH 0020 | BATCH  017 /  021 | DICE_LOSS  0.2402\n",
        "VALID || EPOCH 0020 | BATCH  018 /  021 | DICE_LOSS  0.2370\n",
        "VALID || EPOCH 0020 | BATCH  019 /  021 | DICE_LOSS  0.2446\n",
        "VALID || EPOCH 0020 | BATCH  020 /  021 | DICE_LOSS  0.2444\n",
        "VALID || EPOCH 0020 | BATCH  021 /  021 | DICE_LOSS  0.2418\n",
        "```"
      ],
      "metadata": {
        "id": "AZ5JiTZ1-Sp3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yljPDLtm0zuo"
      },
      "outputs": [],
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{val_idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{val_idx:03d}.png'), label_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case 1-1\n",
        "\n",
        "*변경된 적용사항*\n",
        "1. Black boundary 제거\n",
        "2. Lab space의 l, a 채널 CLAHE Contrast 조정\n",
        "3. BGR의 Green channel을 CLAHE Contrast 조정\n",
        "4. Right/Bottom으로 bordering to size (384, 512) == (y, x)"
      ],
      "metadata": {
        "id": "yEYMmQWNAmdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Performance\n",
        "\n",
        "```\n",
        "Input size =  torch.Size([8, 1, 384, 512])\n",
        "\n",
        "TRAIN || EPOCH 0020 | BATCH  079 /  083 | DICE LOSS  0.2337\n",
        "TRAIN || EPOCH 0020 | BATCH  080 /  083 | DICE LOSS  0.2344\n",
        "TRAIN || EPOCH 0020 | BATCH  081 /  083 | DICE LOSS  0.2339\n",
        "TRAIN || EPOCH 0020 | BATCH  082 /  083 | DICE LOSS  0.2340\n",
        "TRAIN || EPOCH 0020 | BATCH  083 /  083 | DICE LOSS  0.2340\n",
        "\n",
        "VALID || EPOCH 0020 | BATCH  017 /  021 | DICE_LOSS  0.2312\n",
        "VALID || EPOCH 0020 | BATCH  018 /  021 | DICE_LOSS  0.2324\n",
        "VALID || EPOCH 0020 | BATCH  019 /  021 | DICE_LOSS  0.2281\n",
        "VALID || EPOCH 0020 | BATCH  020 /  021 | DICE_LOSS  0.2353\n",
        "VALID || EPOCH 0020 | BATCH  021 /  021 | DICE_LOSS  0.2403\n",
        "```"
      ],
      "metadata": {
        "id": "KueS-p8IBOua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "# ============================== Added code ==============================\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "# =======================================================================\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "# ============================== Added code ==============================\n",
        "    # Pad image\n",
        "    pad_right = int((NORM_INPUT_W_SIZE - width))\n",
        "    pad_bottom = int((NORM_INPUT_H_SIZE - height))\n",
        "\n",
        "    green_image = cv2.copyMakeBorder(green_image,\n",
        "                               0,\n",
        "                               pad_bottom,\n",
        "                               0,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               0,\n",
        "                               pad_bottom,\n",
        "                               0,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "# =======================================================================\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "# ============================== Added code ==============================\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "# =======================================================================\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "# ============================== Added code ==============================\n",
        "    # Pad image\n",
        "    pad_right = int((NORM_INPUT_W_SIZE - width))\n",
        "    pad_bottom = int((NORM_INPUT_H_SIZE - height))\n",
        "\n",
        "    green_image = cv2.copyMakeBorder(green_image,\n",
        "                               0,\n",
        "                               pad_bottom,\n",
        "                               0,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               0,\n",
        "                               pad_bottom,\n",
        "                               0,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "# =======================================================================\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{val_idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{val_idx:03d}.png'), label_image)"
      ],
      "metadata": {
        "id": "JJd7EGcJBGpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhgq7zplcjoh"
      },
      "source": [
        "### Case 2\n",
        "\n",
        "*적용사항*\n",
        "1. 불필요 Black boundary 제거\n",
        "2. Lab space의 l, a 채널 CLAHE Contrast 조정\n",
        "3. BGR의 Green channel을 CLAHE Contrast 조정\n",
        "4. Resize input/output to 384*512 (y, x)\n",
        "> UNet 아키텍쳐 입력 이미지 사이즈"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Performance\n",
        "\n",
        "```\n",
        "Input size =  torch.Size([8, 1, 384, 512])\n",
        "\n",
        "TRAIN || EPOCH 0020 | BATCH  079 /  083 | DICE LOSS  0.2960\n",
        "TRAIN || EPOCH 0020 | BATCH  080 /  083 | DICE LOSS  0.2947\n",
        "TRAIN || EPOCH 0020 | BATCH  081 /  083 | DICE LOSS  0.2943\n",
        "TRAIN || EPOCH 0020 | BATCH  082 /  083 | DICE LOSS  0.2956\n",
        "TRAIN || EPOCH 0020 | BATCH  083 /  083 | DICE LOSS  0.2950\n",
        "\n",
        "VALID || EPOCH 0020 | BATCH  017 /  021 | DICE_LOSS  0.2981\n",
        "VALID || EPOCH 0020 | BATCH  018 /  021 | DICE_LOSS  0.2936\n",
        "VALID || EPOCH 0020 | BATCH  019 /  021 | DICE_LOSS  0.2851\n",
        "VALID || EPOCH 0020 | BATCH  020 /  021 | DICE_LOSS  0.2860\n",
        "VALID || EPOCH 0020 | BATCH  021 /  021 | DICE_LOSS  0.2872\n",
        "```"
      ],
      "metadata": {
        "id": "bUaRzvyV-V-D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecWAX5SGg4Hw"
      },
      "outputs": [],
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "# ============================== Added code ==============================\n",
        "    if width > height: height = width\n",
        "    else: width = height\n",
        "# =======================================================================\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "# ============================== Added code ==============================\n",
        "    # Resize image\n",
        "    green_image = cv2.resize(green_image, \n",
        "                             (NORM_INPUT_W_SIZE, NORM_INPUT_H_SIZE),\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        "    label_image = cv2.resize(label_image, \n",
        "                             (NORM_INPUT_W_SIZE, NORM_INPUT_H_SIZE),\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        "# =======================================================================\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "# ============================== Added code ==============================\n",
        "    if width > height: height = width\n",
        "    else: width = height\n",
        "# =======================================================================\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "# ============================== Added code ==============================\n",
        "    # Resize image\n",
        "    green_image = cv2.resize(green_image, \n",
        "                             (NORM_INPUT_W_SIZE, NORM_INPUT_H_SIZE),\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        "    label_image = cv2.resize(label_image, \n",
        "                             (NORM_INPUT_W_SIZE, NORM_INPUT_H_SIZE),\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        "# =======================================================================\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{val_idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{val_idx:03d}.png'), label_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWxTg5BSop6e"
      },
      "source": [
        "### Case 3 (Local SOTA)\n",
        "\n",
        "*적용사항*\n",
        "1. Black boundary 모두 제거\n",
        "2. Lab space의 l, a 채널 CLAHE Contrast 조정\n",
        "3. BGR의 Green channel을 CLAHE Contrast 조정\n",
        "4. Bordering(cv2.copymakeborder CONSTANT) input/output to 384*512 (y, x)\n",
        "> UNet 아키텍쳐 입력 이미지 사이즈"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Performance\n",
        "\n",
        "```\n",
        "Input size =  torch.Size([8, 1, 384, 512])\n",
        "\n",
        "TRAIN || EPOCH 0020 | BATCH  079 /  083 | DICE LOSS  0.2293\n",
        "TRAIN || EPOCH 0020 | BATCH  080 /  083 | DICE LOSS  0.2289\n",
        "TRAIN || EPOCH 0020 | BATCH  081 /  083 | DICE LOSS  0.2284\n",
        "TRAIN || EPOCH 0020 | BATCH  082 /  083 | DICE LOSS  0.2279\n",
        "TRAIN || EPOCH 0020 | BATCH  083 /  083 | DICE LOSS  0.2279\n",
        "\n",
        "VALID || EPOCH 0020 | BATCH  017 /  021 | DICE_LOSS  0.2322\n",
        "VALID || EPOCH 0020 | BATCH  018 /  021 | DICE_LOSS  0.2458\n",
        "VALID || EPOCH 0020 | BATCH  019 /  021 | DICE_LOSS  0.2446\n",
        "VALID || EPOCH 0020 | BATCH  020 /  021 | DICE_LOSS  0.2419\n",
        "VALID || EPOCH 0020 | BATCH  021 /  021 | DICE_LOSS  0.2499\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "1dFrSvaE-aBx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBf58SJVo5SY"
      },
      "outputs": [],
      "source": [
        "# Set load image dir path\n",
        "image_path = RAW_DATA_PATH + WOUND_TRAIN_PATH\n",
        "label_path = RAW_DATA_PATH + WOUND_TEST_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val:test = 6:2:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size):\n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "# ============================== Added code ==============================\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_H_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_H_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_W_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_W_SIZE - total_width)\n",
        "\n",
        "    green_image = cv2.copyMakeBorder(green_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "# =======================================================================\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "# ============================== Added code ==============================\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_H_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_H_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_W_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_W_SIZE - total_width)\n",
        "\n",
        "    green_image = cv2.copyMakeBorder(green_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "# =======================================================================\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{val_idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{val_idx:03d}.png'), label_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case 3-1\n",
        "\n",
        "3. BGR의 Green channel을 CLAHE Contrast 조정\n",
        "> 변경) Blue constrast down, Red/Green contrast up"
      ],
      "metadata": {
        "id": "L6R_SgbCtWFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Performance\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Input size =  torch.Size([8, 1, 384, 512])\n",
        "\n",
        "TRAIN || EPOCH 0020 | BATCH  079 /  083 | DICE LOSS  0.2519\n",
        "TRAIN || EPOCH 0020 | BATCH  080 /  083 | DICE LOSS  0.2507\n",
        "TRAIN || EPOCH 0020 | BATCH  081 /  083 | DICE LOSS  0.2511\n",
        "TRAIN || EPOCH 0020 | BATCH  082 /  083 | DICE LOSS  0.2521\n",
        "TRAIN || EPOCH 0020 | BATCH  083 /  083 | DICE LOSS  0.2513\n",
        "\n",
        "VALID || EPOCH 0020 | BATCH  017 /  021 | DICE_LOSS  0.3144\n",
        "VALID || EPOCH 0020 | BATCH  018 /  021 | DICE_LOSS  0.3115\n",
        "VALID || EPOCH 0020 | BATCH  019 /  021 | DICE_LOSS  0.3074\n",
        "VALID || EPOCH 0020 | BATCH  020 /  021 | DICE_LOSS  0.3134\n",
        "VALID || EPOCH 0020 | BATCH  021 /  021 | DICE_LOSS  0.3137\n",
        "\n",
        "Best loss in Train =  0.2499\n",
        "Best loss in Validation =  0.2536\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "jjU9wlcdtZEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "# ============================ Modified code ============================\n",
        "    enhanced_lab = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_bgr = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_bgr)\n",
        "\n",
        "    b_alpha = -1.5\n",
        "    g_alpha = 0.5\n",
        "    r_alpha = 1.5\n",
        "\n",
        "    blue_image = np.clip((1 + b_alpha)*blue_image - (128 * b_alpha), 0, 255).astype(np.uint8)\n",
        "    green_image = np.clip((1 + g_alpha)*green_image - (128 * g_alpha), 0, 255).astype(np.uint8)\n",
        "    red_image = np.clip((1 + r_alpha)*red_image - (128 * r_alpha), 0, 255).astype(np.uint8)\n",
        "\n",
        "    enhanced_bgr = cv2.merge((blue_image, green_image, red_image))\n",
        "    input_image = cv2.cvtColor(enhanced_bgr, cv2.COLOR_BGR2GRAY)\n",
        "# =======================================================================\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_H_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_H_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_W_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_W_SIZE - total_width)\n",
        "\n",
        "    input_image = cv2.copyMakeBorder(input_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "\n",
        "    cv2_imshow(input_image)\n",
        "    imshow_waitkey_enter(label_image)\n",
        "    \n",
        "    # Save normalized image\n",
        "    #cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), input_image)\n",
        "    #cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "# ============================ Modified code ============================\n",
        "    enhanced_lab = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_bgr = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_bgr)\n",
        "\n",
        "    b_alpha = -0.5\n",
        "    g_alpha = 2.5\n",
        "    r_alpha = 1.5\n",
        "\n",
        "    blue_image = np.clip((1 + b_alpha)*blue_image - (128 * b_alpha), 0, 255).astype(np.uint8)\n",
        "    green_image = np.clip((1 + g_alpha)*green_image - (128 * g_alpha), 0, 255).astype(np.uint8)\n",
        "    red_image = np.clip((1 + r_alpha)*red_image - (128 * r_alpha), 0, 255).astype(np.uint8)\n",
        "\n",
        "    enhanced_bgr = cv2.merge((blue_image, green_image, red_image))\n",
        "    gray_input = cv2.cvtColor(enhanced_bgr, cv2.COLOR_BGR2GRAY)\n",
        "# =======================================================================\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_H_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_H_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_W_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_W_SIZE - total_width)\n",
        "\n",
        "    gray_input = cv2.copyMakeBorder(gray_input,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{val_idx:03d}.png'), gray_input)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{val_idx:03d}.png'), label_image)"
      ],
      "metadata": {
        "id": "9LkCDwUEteYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case 3-2"
      ],
      "metadata": {
        "id": "Lj-miTM3VHGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Blurring (Smoothing)\n",
        "> Border 전에 양방향 필터 블러링(스무딩) 단계 추가\n",
        "\n",
        "```\n",
        "Input size =  torch.Size([8, 1, 384, 512])\n",
        "\n",
        "TRAIN || EPOCH 0020 | BATCH  079 /  083 | DICE LOSS  0.2479\n",
        "TRAIN || EPOCH 0020 | BATCH  080 /  083 | DICE LOSS  0.2488\n",
        "TRAIN || EPOCH 0020 | BATCH  081 /  083 | DICE LOSS  0.2475\n",
        "TRAIN || EPOCH 0020 | BATCH  082 /  083 | DICE LOSS  0.2472\n",
        "TRAIN || EPOCH 0020 | BATCH  083 /  083 | DICE LOSS  0.2485\n",
        "\n",
        "VALID || EPOCH 0020 | BATCH  017 /  021 | DICE_LOSS  0.3442\n",
        "VALID || EPOCH 0020 | BATCH  018 /  021 | DICE_LOSS  0.3530\n",
        "VALID || EPOCH 0020 | BATCH  019 /  021 | DICE_LOSS  0.3493\n",
        "VALID || EPOCH 0020 | BATCH  020 /  021 | DICE_LOSS  0.3476\n",
        "VALID || EPOCH 0020 | BATCH  021 /  021 | DICE_LOSS  0.3434\n",
        "\n",
        "Best loss in Train =  0.2429\n",
        "Best loss in Validation =  0.2410\n",
        "```"
      ],
      "metadata": {
        "id": "3Dsp4O3eynj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "# ============================== Added code ==============================\n",
        "    # Bluring\n",
        "    green_image = cv2.bilateralFilter(green_image, -1, 75, 75)\n",
        "# =======================================================================\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_H_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_H_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_W_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_W_SIZE - total_width)\n",
        "\n",
        "    green_image = cv2.copyMakeBorder(green_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "# ============================== Added code ==============================\n",
        "    # Bluring\n",
        "    green_image = cv2.bilateralFilter(green_image, -1, 75, 75)\n",
        "# =======================================================================\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_H_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_H_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_W_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_W_SIZE - total_width)\n",
        "\n",
        "    green_image = cv2.copyMakeBorder(green_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{val_idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{val_idx:03d}.png'), label_image)"
      ],
      "metadata": {
        "id": "_IjSEgPSVvCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Sharpening\n",
        "> Border 전에 샤프닝 단계 추가\n",
        "\n",
        "```\n",
        "Input size =  torch.Size([8, 1, 384, 512])\n",
        "\n",
        "TRAIN || EPOCH 0020 | BATCH  079 /  083 | DICE LOSS  0.2186\n",
        "TRAIN || EPOCH 0020 | BATCH  080 /  083 | DICE LOSS  0.2180\n",
        "TRAIN || EPOCH 0020 | BATCH  081 /  083 | DICE LOSS  0.2177\n",
        "TRAIN || EPOCH 0020 | BATCH  082 /  083 | DICE LOSS  0.2180\n",
        "TRAIN || EPOCH 0020 | BATCH  083 /  083 | DICE LOSS  0.2181\n",
        "\n",
        "VALID || EPOCH 0020 | BATCH  017 /  021 | DICE_LOSS  0.3035\n",
        "VALID || EPOCH 0020 | BATCH  018 /  021 | DICE_LOSS  0.3050\n",
        "VALID || EPOCH 0020 | BATCH  019 /  021 | DICE_LOSS  0.3037\n",
        "VALID || EPOCH 0020 | BATCH  020 /  021 | DICE_LOSS  0.2996\n",
        "VALID || EPOCH 0020 | BATCH  021 /  021 | DICE_LOSS  0.3051\n",
        "\n",
        "Best loss in Train =  0.2181\n",
        "Best loss in Validation =  0.2431\n",
        "```"
      ],
      "metadata": {
        "id": "ZKAEkAJ3zXqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "# ============================== Added code ==============================\n",
        "    # Sharpening\n",
        "    sharp_kernel = np.array([[0, -1, 0],\n",
        "                             [-1, 9, -1],\n",
        "                             [0, -1, 0]])\n",
        "    sharp_image = cv2.filter2D(green_image, -1, sharp_kernel)\n",
        "\n",
        "    green_image = cv2.addWeighted(green_image, 0.75, sharp_image, 0.25, 1)\n",
        "# =======================================================================\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_H_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_H_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_W_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_W_SIZE - total_width)\n",
        "\n",
        "    green_image = cv2.copyMakeBorder(green_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "    \n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "# ============================== Added code ==============================\n",
        "    # Sharpening\n",
        "    sharp_kernel = np.array([[0, -1, 0],\n",
        "                             [-1, 9, -1],\n",
        "                             [0, -1, 0]])\n",
        "    sharp_image = cv2.filter2D(green_image, -1, sharp_kernel)\n",
        "\n",
        "    green_image = cv2.addWeighted(green_image, 0.75, sharp_image, 0.25, 1)\n",
        "# =======================================================================\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_H_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_H_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_W_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_W_SIZE - total_width)\n",
        "\n",
        "    green_image = cv2.copyMakeBorder(green_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{val_idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{val_idx:03d}.png'), label_image)"
      ],
      "metadata": {
        "id": "RlgTJQUymJoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Blurring (Smoothing) + Sharpening\n",
        "> Bordering 전에 스무딩 후 샤프닝 단계 추가\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Input size =  torch.Size([8, 1, 384, 512])\n",
        "\n",
        "TRAIN || EPOCH 0020 | BATCH  078 /  083 | DICE LOSS  0.2372\n",
        "TRAIN || EPOCH 0020 | BATCH  079 /  083 | DICE LOSS  0.2362\n",
        "TRAIN || EPOCH 0020 | BATCH  080 /  083 | DICE LOSS  0.2357\n",
        "TRAIN || EPOCH 0020 | BATCH  081 /  083 | DICE LOSS  0.2344\n",
        "TRAIN || EPOCH 0020 | BATCH  082 /  083 | DICE LOSS  0.2346\n",
        "\n",
        "VALID || EPOCH 0020 | BATCH  017 /  021 | DICE_LOSS  0.2690\n",
        "VALID || EPOCH 0020 | BATCH  018 /  021 | DICE_LOSS  0.2643\n",
        "VALID || EPOCH 0020 | BATCH  019 /  021 | DICE_LOSS  0.2591\n",
        "VALID || EPOCH 0020 | BATCH  020 /  021 | DICE_LOSS  0.2603\n",
        "VALID || EPOCH 0020 | BATCH  021 /  021 | DICE_LOSS  0.2716\n",
        "\n",
        "Best loss in Train =  0.2346\n",
        "Best loss in Validation =  0.2500\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "2OH8okJxzf28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "# ============================== Added code ==============================\n",
        "    # Bluring\n",
        "    green_image = cv2.bilateralFilter(green_image, -1, 75, 75)\n",
        "\n",
        "    # Sharpening\n",
        "    sharp_kernel = np.array([[0, -1, 0],\n",
        "                             [-1, 9, -1],\n",
        "                             [0, -1, 0]])\n",
        "    sharp_image = cv2.filter2D(green_image, -1, sharp_kernel)\n",
        "\n",
        "    green_image = cv2.addWeighted(green_image, 0.75, sharp_image, 0.25, 1)\n",
        "# =======================================================================\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_H_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_H_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_W_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_W_SIZE - total_width)\n",
        "\n",
        "    green_image = cv2.copyMakeBorder(green_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "# ============================== Added code ==============================\n",
        "    # Bluring\n",
        "    green_image = cv2.bilateralFilter(green_image, -1, 75, 75)\n",
        "\n",
        "    # Sharpening\n",
        "    sharp_kernel = np.array([[0, -1, 0],\n",
        "                             [-1, 9, -1],\n",
        "                             [0, -1, 0]])\n",
        "    sharp_image = cv2.filter2D(green_image, -1, sharp_kernel)\n",
        "\n",
        "    green_image = cv2.addWeighted(green_image, 0.75, sharp_image, 0.25, 1)\n",
        "# =======================================================================\n",
        "\n",
        "    # Pad image\n",
        "    pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "    pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "    pad_top = pad_height\n",
        "    pad_bottom = pad_height\n",
        "    pad_left = pad_width\n",
        "    pad_right = pad_width\n",
        "\n",
        "    total_height = height + (pad_height * 2)\n",
        "    total_width = width + (pad_width * 2)\n",
        "        \n",
        "    if total_height > NORM_INPUT_H_SIZE:\n",
        "        pad_top = pad_top - (total_height - NORM_INPUT_H_SIZE)\n",
        "    elif total_height < NORM_INPUT_H_SIZE:\n",
        "        pad_bottom = pad_bottom + (NORM_INPUT_H_SIZE - total_height)\n",
        "\n",
        "    if total_width > NORM_INPUT_W_SIZE:\n",
        "        pad_left = pad_left - (total_width - NORM_INPUT_W_SIZE)\n",
        "    elif total_width < NORM_INPUT_W_SIZE:\n",
        "        pad_right = pad_right + (NORM_INPUT_W_SIZE - total_width)\n",
        "\n",
        "    green_image = cv2.copyMakeBorder(green_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{val_idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{val_idx:03d}.png'), label_image)"
      ],
      "metadata": {
        "id": "Owb4YgwpzoaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case 4\n",
        "\n",
        "*적용사항*\n",
        "1. Black boundary 모두 제거\n",
        "2. Lab space의 l, a 채널 CLAHE Contrast 조정\n",
        "3. BGR의 Green channel을 CLAHE Contrast 조정\n",
        "4. Resize input/label width to 384 or height to 512 (비율 유지)\n",
        "5. Bordering(cv2.copymakeborder CONSTANT) input/output to 384*512 (y, x)\n",
        "> UNet 아키텍쳐 입력 이미지 사이즈"
      ],
      "metadata": {
        "id": "1Yxo1r0o1gQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Performance\n",
        "\n",
        "```\n",
        "Input size =  torch.Size([8, 1, 384, 512])\n",
        "\n",
        "TRAIN || EPOCH 0020 | BATCH  079 /  083 | DICE LOSS  0.2895\n",
        "TRAIN || EPOCH 0020 | BATCH  080 /  083 | DICE LOSS  0.2880\n",
        "TRAIN || EPOCH 0020 | BATCH  081 /  083 | DICE LOSS  0.2888\n",
        "TRAIN || EPOCH 0020 | BATCH  082 /  083 | DICE LOSS  0.2885\n",
        "TRAIN || EPOCH 0020 | BATCH  083 /  083 | DICE LOSS  0.2902\n",
        "\n",
        "VALID || EPOCH 0020 | BATCH  017 /  021 | DICE_LOSS  0.2928\n",
        "VALID || EPOCH 0020 | BATCH  018 /  021 | DICE_LOSS  0.2864\n",
        "VALID || EPOCH 0020 | BATCH  019 /  021 | DICE_LOSS  0.2852\n",
        "VALID || EPOCH 0020 | BATCH  020 /  021 | DICE_LOSS  0.2823\n",
        "VALID || EPOCH 0020 | BATCH  021 /  021 | DICE_LOSS  0.2844\n",
        "```"
      ],
      "metadata": {
        "id": "LmRR68162Q_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "# ============================== Added code ==============================\n",
        "    # Resize images\n",
        "    resize_scale_w = NORM_INPUT_W_SIZE / width\n",
        "    resize_scale_h = NORM_INPUT_H_SIZE / height\n",
        "\n",
        "    resize_criteria = -1\n",
        "    if resize_scale_w < resize_scale_h:\n",
        "        resize_scale = resize_scale_w\n",
        "        resize_criteria = 0\n",
        "    else:\n",
        "        resize_scale = resize_scale_h\n",
        "        resize_criteria = 1\n",
        "\n",
        "    green_image = cv2.resize(green_image,\n",
        "                             (0, 0),\n",
        "                             fx=resize_scale,\n",
        "                             fy=resize_scale,\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        "    label_image = cv2.resize(label_image, \n",
        "                             (0, 0),\n",
        "                             fx=resize_scale,\n",
        "                             fy=resize_scale,\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        " \n",
        "    width = round(width * resize_scale)\n",
        "    height = round(height * resize_scale)\n",
        "# =======================================================================\n",
        "\n",
        "# ============================ Modified code ============================\n",
        "    # Pad image\n",
        "    pad_top = 0\n",
        "    pad_bottom = 0\n",
        "    pad_left = 0\n",
        "    pad_right = 0\n",
        "\n",
        "    if resize_criteria == 0:\n",
        "        pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "        pad_top = pad_height\n",
        "        pad_bottom = pad_height\n",
        "        \n",
        "        total_height = height + (pad_height * 2)\n",
        "\n",
        "        if total_height > NORM_INPUT_H_SIZE:\n",
        "            pad_top = pad_top - (total_height - NORM_INPUT_H_SIZE)\n",
        "        elif total_height < NORM_INPUT_H_SIZE:\n",
        "            pad_bottom = pad_bottom + (NORM_INPUT_H_SIZE - total_height)\n",
        "\n",
        "    elif resize_criteria == 1:\n",
        "        pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "\n",
        "        pad_left = pad_width\n",
        "        pad_right = pad_width\n",
        "\n",
        "        total_width = width + (pad_width * 2)\n",
        "\n",
        "        if total_width > NORM_INPUT_W_SIZE:\n",
        "            pad_left = pad_left - (total_width - NORM_INPUT_W_SIZE)\n",
        "        elif total_width < NORM_INPUT_W_SIZE:\n",
        "            pad_right = pad_right + (NORM_INPUT_W_SIZE - total_width)\n",
        "\n",
        "    else:\n",
        "        print(\"Resize Error, [ocurred index] = \", idx)\n",
        "        break\n",
        "    \n",
        "    green_image = cv2.copyMakeBorder(green_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "# =======================================================================\n",
        "    \n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    \n",
        "    # Split each BGR\n",
        "    blue_image, green_image, red_image = cv2.split(enhanced_image)\n",
        "\n",
        "    green_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    green_image = green_clahe.apply(green_image)\n",
        "\n",
        "# ============================== Added code ==============================\n",
        "    # Resize images\n",
        "    resize_scale_w = NORM_INPUT_W_SIZE / width\n",
        "    resize_scale_h = NORM_INPUT_H_SIZE / height\n",
        "\n",
        "    resize_criteria = -1\n",
        "    if resize_scale_w < resize_scale_h:\n",
        "        resize_scale = resize_scale_w\n",
        "        resize_criteria = 0\n",
        "    else:\n",
        "        resize_scale = resize_scale_h\n",
        "        resize_criteria = 1\n",
        "\n",
        "    green_image = cv2.resize(green_image,\n",
        "                             (0, 0),\n",
        "                             fx=resize_scale,\n",
        "                             fy=resize_scale,\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        "    label_image = cv2.resize(label_image, \n",
        "                             (0, 0),\n",
        "                             fx=resize_scale,\n",
        "                             fy=resize_scale,\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        " \n",
        "    width = round(width * resize_scale)\n",
        "    height = round(height * resize_scale)\n",
        "# =======================================================================\n",
        "\n",
        "# ============================ Modified code ============================\n",
        "    # Pad image\n",
        "    pad_top = 0\n",
        "    pad_bottom = 0\n",
        "    pad_left = 0\n",
        "    pad_right = 0\n",
        "\n",
        "    if resize_criteria == 0:\n",
        "        pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "        pad_top = pad_height\n",
        "        pad_bottom = pad_height\n",
        "        \n",
        "        total_height = height + (pad_height * 2)\n",
        "\n",
        "        if total_height > NORM_INPUT_H_SIZE:\n",
        "            pad_top = pad_top - (total_height - NORM_INPUT_H_SIZE)\n",
        "        elif total_height < NORM_INPUT_H_SIZE:\n",
        "            pad_bottom = pad_bottom + (NORM_INPUT_H_SIZE - total_height)\n",
        "\n",
        "    elif resize_criteria == 1:\n",
        "        pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "\n",
        "        pad_left = pad_width\n",
        "        pad_right = pad_width\n",
        "\n",
        "        total_width = width + (pad_width * 2)\n",
        "\n",
        "        if total_width > NORM_INPUT_W_SIZE:\n",
        "            pad_left = pad_left - (total_width - NORM_INPUT_W_SIZE)\n",
        "        elif total_width < NORM_INPUT_W_SIZE:\n",
        "            pad_right = pad_right + (NORM_INPUT_W_SIZE - total_width)\n",
        "\n",
        "    else:\n",
        "        print(\"Resize Error, [ocurred index] = \", idx)\n",
        "        break\n",
        "    \n",
        "    green_image = cv2.copyMakeBorder(green_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "# =======================================================================\n",
        "    \n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{val_idx:03d}.png'), green_image)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{val_idx:03d}.png'), label_image)"
      ],
      "metadata": {
        "id": "FLbx55hB2Rrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case 5\n",
        "\n",
        "*적용사항*\n",
        "1. Black boundary 모두 제거\n",
        "2. Lab space의 l, a 채널 CLAHE Contrast 조정\n",
        "3. K-Means(k=3)를 이용한 이미지 분류\n",
        "4. Resize input/label width to 384 or height to 512 (비율 유지)\n",
        "5. Bordering(cv2.copymakeborder CONSTANT) input/output to 384*512 (y, x)\n",
        "> UNet 아키텍쳐 입력 이미지 사이즈"
      ],
      "metadata": {
        "id": "LqgHSjqj-dBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Performance\n",
        "\n",
        "```\n",
        "Input size =  torch.Size([8, 1, 384, 512])\n",
        "\n",
        "TRAIN || EPOCH 0020 | BATCH  079 /  083 | DICE LOSS  0.3189\n",
        "TRAIN || EPOCH 0020 | BATCH  080 /  083 | DICE LOSS  0.3178\n",
        "TRAIN || EPOCH 0020 | BATCH  081 /  083 | DICE LOSS  0.3177\n",
        "TRAIN || EPOCH 0020 | BATCH  082 /  083 | DICE LOSS  0.3207\n",
        "TRAIN || EPOCH 0020 | BATCH  083 /  083 | DICE LOSS  0.3198\n",
        "\n",
        "VALID || EPOCH 0020 | BATCH  017 /  021 | DICE_LOSS  0.3028\n",
        "VALID || EPOCH 0020 | BATCH  018 /  021 | DICE_LOSS  0.2980\n",
        "VALID || EPOCH 0020 | BATCH  019 /  021 | DICE_LOSS  0.3053\n",
        "VALID || EPOCH 0020 | BATCH  020 /  021 | DICE_LOSS  0.3081\n",
        "VALID || EPOCH 0020 | BATCH  021 /  021 | DICE_LOSS  0.3027\n",
        "```"
      ],
      "metadata": {
        "id": "FD1t6uTb_73Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set load image dir path\n",
        "image_path = MODEL_PATH + RAW_TRAIN_SET_PATH + IMAGES_PATH\n",
        "label_path = MODEL_PATH + RAW_TRAIN_SET_PATH + LABELS_PATH\n",
        "\n",
        "# Load raw train images\n",
        "train_files = os.listdir(image_path)\n",
        "train_files.sort()\n",
        "\n",
        "# Load raw label images\n",
        "train_label_files = os.listdir(label_path)\n",
        "train_label_files.sort()\n",
        "\n",
        "# Divide train:val = 8:2\n",
        "dataset_size = len(train_files)\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Create processed scar image for train\n",
        "for idx in range(train_size): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    \n",
        "# ============================== Added code ==============================\n",
        "    # Clustering use K-means\n",
        "    cluster_data = enhanced_image.reshape((-1, 3)).astype(np.float32)\n",
        "    k = 3\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS|cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0) # 최대 10번 반복하고 1픽셀 이하로 움직이면 종료\n",
        "    ret, c_label, center = cv2.kmeans(cluster_data, \n",
        "                                    k, \n",
        "                                    None, \n",
        "                                    criteria,\n",
        "                                    10,\n",
        "                                    cv2.KMEANS_RANDOM_CENTERS)\n",
        "    \n",
        "    center = np.uint8(center)\n",
        "    kmeans_image = center[c_label.flatten()]\n",
        "    kmeans_image = kmeans_image.reshape((enhanced_image.shape))\n",
        "    kmeans_image = cv2.cvtColor(kmeans_image, cv2.COLOR_BGR2GRAY)\n",
        "# =======================================================================\n",
        "\n",
        "# ============================ Modified code ============================\n",
        "    # Resize images\n",
        "    resize_scale_w = NORM_INPUT_W_SIZE / width\n",
        "    resize_scale_h = NORM_INPUT_H_SIZE / height\n",
        "\n",
        "    resize_criteria = -1\n",
        "    if resize_scale_w < resize_scale_h:\n",
        "        resize_scale = resize_scale_w\n",
        "        resize_criteria = 0\n",
        "    else:\n",
        "        resize_scale = resize_scale_h\n",
        "        resize_criteria = 1\n",
        "\n",
        "    kmeans_image = cv2.resize(kmeans_image,\n",
        "                             (0, 0),\n",
        "                             fx=resize_scale,\n",
        "                             fy=resize_scale,\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        "    label_image = cv2.resize(label_image, \n",
        "                             (0, 0),\n",
        "                             fx=resize_scale,\n",
        "                             fy=resize_scale,\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        " \n",
        "    width = round(width * resize_scale)\n",
        "    height = round(height * resize_scale)\n",
        "# =======================================================================\n",
        "\n",
        "    # Pad image\n",
        "    pad_top = 0\n",
        "    pad_bottom = 0\n",
        "    pad_left = 0\n",
        "    pad_right = 0\n",
        "\n",
        "    if resize_criteria == 0:\n",
        "        pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "        pad_top = pad_height\n",
        "        pad_bottom = pad_height\n",
        "        \n",
        "        total_height = height + (pad_height * 2)\n",
        "\n",
        "        if total_height > NORM_INPUT_H_SIZE:\n",
        "            pad_top = pad_top - (total_height - NORM_INPUT_H_SIZE)\n",
        "        elif total_height < NORM_INPUT_H_SIZE:\n",
        "            pad_bottom = pad_bottom + (NORM_INPUT_H_SIZE - total_height)\n",
        "\n",
        "    elif resize_criteria == 1:\n",
        "        pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "\n",
        "        pad_left = pad_width\n",
        "        pad_right = pad_width\n",
        "\n",
        "        total_width = width + (pad_width * 2)\n",
        "\n",
        "        if total_width > NORM_INPUT_W_SIZE:\n",
        "            pad_left = pad_left - (total_width - NORM_INPUT_W_SIZE)\n",
        "        elif total_width < NORM_INPUT_W_SIZE:\n",
        "            pad_right = pad_right + (NORM_INPUT_W_SIZE - total_width)\n",
        "\n",
        "    else:\n",
        "        print(\"Resize Error, [ocurred index] = \", idx)\n",
        "        break\n",
        "    \n",
        "    kmeans_image = cv2.copyMakeBorder(kmeans_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "    \n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(train_dir, f'scar_{idx:03d}.png'), kmeans_image)\n",
        "    cv2.imwrite(os.path.join(train_dir, f'label_{idx:03d}.png'), label_image)\n",
        "\n",
        "# Create processed scar image for validation\n",
        "for val_idx, idx in enumerate(range(train_size, dataset_size)): \n",
        "    # Access train set\n",
        "    os.chdir(image_path)\n",
        "    image = cv2.imread(train_files[idx])\n",
        "\n",
        "    # Access label set\n",
        "    os.chdir(label_path)\n",
        "    label_image = cv2.imread(train_label_files[idx])\n",
        "\n",
        "    # Get grayscale image\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Cut black boundary on value image\n",
        "    contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    x, y, width, height = cv2.boundingRect(contours[0])\n",
        "\n",
        "    image = image[y:y+height, x:x+width]\n",
        "    label_image = label_image[y:y+height, x:x+width]\n",
        "\n",
        "    # Equalize histogram used by LAB and BGR color space for make enhanced image \n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l_image, a_image, b_image = cv2.split(lab_image)\n",
        "\n",
        "    l_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
        "    a_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))\n",
        "    l_image = l_clahe.apply(l_image)\n",
        "    a_image = a_clahe.apply(a_image)\n",
        "\n",
        "    lab_image = cv2.merge((l_image, a_image, b_image))\n",
        "    enhanced_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2BGR)\n",
        "    \n",
        "# ============================== Added code ==============================\n",
        "    # Clustering use K-means\n",
        "    cluster_data = enhanced_image.reshape((-1, 3)).astype(np.float32)\n",
        "    k = 3\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS|cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0) # 최대 10번 반복하고 1픽셀 이하로 움직이면 종료\n",
        "    ret, c_label, center = cv2.kmeans(cluster_data, \n",
        "                                    k, \n",
        "                                    None, \n",
        "                                    criteria,\n",
        "                                    10,\n",
        "                                    cv2.KMEANS_RANDOM_CENTERS)\n",
        "    \n",
        "    center = np.uint8(center)\n",
        "    kmeans_image = center[c_label.flatten()]\n",
        "    kmeans_image = kmeans_image.reshape((enhanced_image.shape))\n",
        "    kmeans_image = cv2.cvtColor(kmeans_image, cv2.COLOR_BGR2GRAY)\n",
        "# =======================================================================\n",
        "\n",
        "# ============================ Modified code ============================\n",
        "    # Resize images\n",
        "    resize_scale_w = NORM_INPUT_W_SIZE / width\n",
        "    resize_scale_h = NORM_INPUT_H_SIZE / height\n",
        "\n",
        "    resize_criteria = -1\n",
        "    if resize_scale_w < resize_scale_h:\n",
        "        resize_scale = resize_scale_w\n",
        "        resize_criteria = 0\n",
        "    else:\n",
        "        resize_scale = resize_scale_h\n",
        "        resize_criteria = 1\n",
        "\n",
        "    kmeans_image = cv2.resize(kmeans_image,\n",
        "                             (0, 0),\n",
        "                             fx=resize_scale,\n",
        "                             fy=resize_scale,\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        "    label_image = cv2.resize(label_image, \n",
        "                             (0, 0),\n",
        "                             fx=resize_scale,\n",
        "                             fy=resize_scale,\n",
        "                             interpolation=cv2.INTER_CUBIC)\n",
        " \n",
        "    width = round(width * resize_scale)\n",
        "    height = round(height * resize_scale)\n",
        "# =======================================================================\n",
        "\n",
        "    # Pad image\n",
        "    pad_top = 0\n",
        "    pad_bottom = 0\n",
        "    pad_left = 0\n",
        "    pad_right = 0\n",
        "\n",
        "    if resize_criteria == 0:\n",
        "        pad_height = int((NORM_INPUT_H_SIZE - height) / 2)\n",
        "\n",
        "        pad_top = pad_height\n",
        "        pad_bottom = pad_height\n",
        "        \n",
        "        total_height = height + (pad_height * 2)\n",
        "\n",
        "        if total_height > NORM_INPUT_H_SIZE:\n",
        "            pad_top = pad_top - (total_height - NORM_INPUT_H_SIZE)\n",
        "        elif total_height < NORM_INPUT_H_SIZE:\n",
        "            pad_bottom = pad_bottom + (NORM_INPUT_H_SIZE - total_height)\n",
        "\n",
        "    elif resize_criteria == 1:\n",
        "        pad_width = int((NORM_INPUT_W_SIZE - width) / 2)\n",
        "\n",
        "        pad_left = pad_width\n",
        "        pad_right = pad_width\n",
        "\n",
        "        total_width = width + (pad_width * 2)\n",
        "\n",
        "        if total_width > NORM_INPUT_W_SIZE:\n",
        "            pad_left = pad_left - (total_width - NORM_INPUT_W_SIZE)\n",
        "        elif total_width < NORM_INPUT_W_SIZE:\n",
        "            pad_right = pad_right + (NORM_INPUT_W_SIZE - total_width)\n",
        "\n",
        "    else:\n",
        "        print(\"Resize Error, [ocurred index] = \", idx)\n",
        "        break\n",
        "    \n",
        "    kmeans_image = cv2.copyMakeBorder(kmeans_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,     # method\n",
        "                               value=[0, 0, 0])         # constant value\n",
        "    label_image = cv2.copyMakeBorder(label_image,\n",
        "                               pad_top,\n",
        "                               pad_bottom,\n",
        "                               pad_left,\n",
        "                               pad_right,\n",
        "                               cv2.BORDER_CONSTANT,\n",
        "                               value=[0, 0, 0])\n",
        "\n",
        "    # Save normalized image\n",
        "    cv2.imwrite(os.path.join(val_dir, f'scar_{idx:03d}.png'), kmeans_image)\n",
        "    cv2.imwrite(os.path.join(val_dir, f'label_{idx:03d}.png'), label_image)"
      ],
      "metadata": {
        "id": "Zgg9Ps9g_Bm3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "W7ranE7jK6DM",
        "AZ5JiTZ1-Sp3",
        "KueS-p8IBOua",
        "3Dsp4O3eynj3",
        "LmRR68162Q_5"
      ],
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNoCBoTMHV0O9cfJTiTukLl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}